{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97dd4909-7d02-4520-80a5-86a9e32773eb",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [1. Get Data](#1.-Get-Data)\n",
    "- [2. Split Data](#2.-Split-Data)\n",
    "- [3. Preprocessing](#3.-Preprocessing)\n",
    "  - [3.1. Clean the Data](#3.1.-Clean-the-Data)\n",
    "  - [3.2. Transform the Data](#3.2.-Transform-the-Data)\n",
    "  - [3.3. Feature Engineering](#3.3.-Feature-Engineering)\n",
    "    - [3.3.1 Feature Engineering - Numerical Features](#331-Feature-Engineering---Numerical-Features)\n",
    "      - [Feature creation](#Feature-creation)\n",
    "      - [Relevance Analysis of Numerical Features](#Relevance-Analysis-of-Numerical-Features)\n",
    "        - [Steps taken](#Steps-taken)\n",
    "        - [Plots Examples](#Plots-Examples)\n",
    "          - [Relevant Feature: Mistakes Dist Ratio](#Relevant-Feature-Mistakes-Dist-Ratio)\n",
    "          - [Non-Relevant Feature: Average Sentence Length](#Non-Relevant-Feature-Average-Sentence-Length)\n",
    "        - [General Observations for Low Relevance Features](#General-Observations-for-Low-Relevance-Features)\n",
    "      - [Correlation Analysis of Numerical Features](#Correlation-Analysis-of-Numerical-Features)\n",
    "    - [3.3.2 Feature Engineering - TF-IDF](#332-Feature-Engineering---TF-IDF)\n",
    "    - [3.3.3 Feature Engineering - Combine Numerical and TF-IDF features](#333-Feature-Engineering---Combine-Numerical-and-TF-IDF-features)\n",
    "    - [3.3.4 Feature Engineering - Word2Vect](#334-Feature-Engineering---Word2Vect)\n",
    "    - [3.3.5 Feature Engineering - BERT](#335-Feature-Engineering---BERT)\n",
    "- [4. Experiments](#4.-Experiments)\n",
    "  - [4.1. Experiment 1 (train models on the initial 17k data set)](#4.1.-Experiment-1-(train-models-on-the-initial-17k-data-set))\n",
    "    - [4.1.1 Experiment Summary](#411-Experiment-Summary)\n",
    "    - [4.1.2 Results](#412-Results)\n",
    "      - [4.1.2.1 Numerical Features Only](#4121-Numerical-Features-Only)\n",
    "      - [4.1.2.2 TF-IDF Features Only](#4122-TF-IDF-Features-Only)\n",
    "      - [4.1.2.3 Combined Numerical and TF-IDF Features](#4123-Combined-Numerical-and-TF-IDF-Features)\n",
    "      - [4.1.2.4 Word2Vec Features Only](#4124-Word2Vec-Features-Only)\n",
    "      - [4.1.2.5 BERT](#4125-BERT)\n",
    "    - [4.1.3 Summary of Results and Recommendations](#413-Summary-of-Results-and-Recommendations)\n",
    "  - [4.2. Experiment 2 (train models on expanded data set by essays with underrepresented scores 1,5,6)](#4.2.-Experiment-2-(train-models-on-expanded-data-set-by-essays-with-underrepresented-scores-1,5,6))\n",
    "    - [4.2.1 Comparison of initial data, preprocessing and model training for Experiment 1 and Experiment 2](#421-Comparison-of-initial-data-preprocessing-and-model-training-for-Experiment-1-and-Experiment-2)\n",
    "    - [4.2.2 Comparison of the results from Experiment 1 and Experiment 2](#422-Comparison-of-the-results-from-Experiment-1-and-Experiment-2)\n",
    "      - [4.2.2.1 Numerical Features Only](#4221-Numerical-Features-Only)\n",
    "      - [4.2.2.2 TF-IDF Features Only](#4222-TF-IDF-Features-Only)\n",
    "      - [4.2.2.3 Combined Numerical and TF-IDF Features](#4223-Combined-Numerical-and-TF-IDF-Features)\n",
    "      - [4.2.2.4 Word2Vec Features Only](#4224-Word2Vec-Features-Only)\n",
    "      - [4.2.2.5 BERT](#4225-BERT)\n",
    "    - [4.2.3 Summary of Results and Recommendations](#423-Summary-of-Results-and-Recommendations)\n",
    "- [5. Dimensionality Reduction](#5.-Dimensionality-Reduction)\n",
    "- [6. Hyperparameter tuning](#6.-Hyperparameter-tuning)\n",
    "- [7. Predict on Unseen Data](#7.-Predict-on-Unseen-Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629210b7-f602-4a9e-aa9f-33ddf7142d4d",
   "metadata": {},
   "source": [
    "## 1. Get Data\n",
    "\n",
    "The dataset for this project comes from the Learning Agency Lab Automated Essay Scoring 2 competition on Kaggle. The goal is to train a model to score student essays, with the evaluation metric being quadratic weighted kappa.\n",
    "\n",
    "The dataset comprises approximately 17,000 student-written argumentative essays, each scored on a scale of 1 to 6. The dataset is split into training and test sets.\n",
    "\n",
    "- **train.csv**: Contains essays and their corresponding scores.\n",
    "  - `essay_id`: The unique ID of the essay.\n",
    "  - `full_text`: The full essay response.\n",
    "  - `score`: The holistic score of the essay on a 1-6 scale.\n",
    "\n",
    "- **test.csv**: Unseen test data on Kaggle side. It has the same fields as train.csv, excluding the score. The rerun test set has approximately 8,000 observations.\n",
    "    - Note: This unseen dataset is located on Kaggle under the hood, and might be used additionally to evaluate model performance on unseen data. In this task, we will split the training data 80/20 to create a test set, which will be used during model testing. \n",
    "\n",
    "- **sample_submission.csv**: A submission file in the correct format.\n",
    "  - `essay_id`: The unique ID of the essay.\n",
    "  - `score`: The predicted holistic score of the essay on a 1-6 scale.\n",
    "\n",
    "For more details, refer to the [Learning Agency Lab Automated Essay Scoring 2 Kaggle Page](https://www.kaggle.com/c/learning-agency-lab-automated-essay-scoring-2).\n",
    "\n",
    "- **Initial EDA**\n",
    "- The dataset contains **17,307 entries** and **3 columns**.\n",
    "- The 'score' column shows an **imbalanced distribution**, with scores 1, 5, and 6 having significantly fewer samples, as illustrated in the plot below:\n",
    "\n",
    "<img src=\"score_distribution_plot.png\" alt=\"Score Distribution\" width=\"800\" height=\"600\"/>\n",
    "\n",
    "## 2. Split Data\n",
    "The data was split into training (80%) and testing (20%) sets using stratified sampling to ensure that the score proportions are maintained in both sets. This approach preserves the original distribution of scores, which is crucial for accurately handling imbalanced datasets (see plot for details).\n",
    "- The `train_split` dataset contains **13,845 entries** and **3 columns**.\n",
    "- The `test_split` dataset contains **3,462 entries** and **3 columns**.\n",
    "\n",
    "<img src=\"score_distribution_train_test_plot.png\" alt=\"Score Distribution Train and Test\" width=\"1200\" height=\"900\"/>\n",
    "\n",
    "## 3. Preprocessing\n",
    "### 3.1 Clean the Data\n",
    "This stage includes handling missing values, removing duplicates, and correcting errors.\n",
    "\n",
    "- **Missing Values:**\n",
    "  - There are no missing values in the `full_text` column.\n",
    "\n",
    "- **Duplicate Detection:**\n",
    "  - Two approaches were utilized to find duplicates:\n",
    "    1. Finding entries that are not exactly identical.\n",
    "        - No duplicates found.\n",
    "    2. TF-IDF vectorization followed by a similarity measure such as cosine similarity.\n",
    "      - **Threshold 0.95:** Returns 1 duplicate.\n",
    "      - **Threshold 0.9:** Returns 2 duplicates.\n",
    "        - Texts found with threshold 0.9 have similar lengths, and scores are the same. The difference (visually defined) is that one essay has a PII placeholder, while another doesn't, e.g., `PROPER_NAME` in one essay and `Luk` in another.\n",
    "      - **Threshold 0.8:** Returns 7 duplicates.\n",
    "        - Texts have different scores, noticeably due to different lengths; hence this option will be skipped. It appears that students might have copied essays from friends and enriched them.\n",
    "    -  See the code in \"Analyzing duplicates\" Notebook\n",
    "- **Replacing PII Placeholders:**\n",
    "  - During duplicate detection, it was noticed that `full_text` contains placeholders for PII data. The volume of placeholders is as follows:\n",
    "    ```\n",
    "    PROPER_NAME      252\n",
    "    EMAIL_ADDRESS      2\n",
    "    STUDENT_NAME       7\n",
    "    OTHER_PII         28\n",
    "    LOCATION_NAME     12\n",
    "    SCHOOL_NAME       14\n",
    "    GENERIC_NAME       1\n",
    "    PHONE_NUMBER       2\n",
    "    STREET_ADDRESS     2\n",
    "    STATE_NAME         1\n",
    "    TEST_NAME          1\n",
    "    CITY_STATE         1\n",
    "    ```\n",
    "\n",
    "  - Given the low frequency of most placeholders, it was decided to skip replacing these placeholders with real names. The effort required to accurately replace these terms may not be justified by the potential (and likely minimal) improvements in model performance.\n",
    "  - See the code in \"Analyzing PII placeholders\" Notebook\n",
    "\n",
    "### 3.2 Transform the Data\n",
    "This stage includes several key steps to prepare the text for machine learning models. The following transformations were applied:\n",
    "\n",
    "- **Standardizing Contractions**: Expanded common contractions using a predefined dictionary.\n",
    "- **Removing HTML Tags**: Eliminated HTML tags to retain only plain text.\n",
    "- **Removing Special Characters and Punctuation**: Cleansed text by removing special characters and punctuation.\n",
    "- **Removing Words with Numbers**: Removed words containing numbers and any trailing 's.\n",
    "- **Removing Stop Words**: Eliminated common stop words.\n",
    "- **Removing Non-ASCII Characters**: Removed non-ASCII characters, including emojis.\n",
    "- **Tokenization and Lemmatization**: Applied NLTK's tokenization and lemmatization using WordNet POS tags.\n",
    "- **Identifying Misspelt Words**: Identified and counted misspelled words using a spell checker.\n",
    "- **Stemming (Removed)**: Initially applied stemming but later removed it in favor of lemmatization for better results.\n",
    "\n",
    "### 3.3. Feature Engineering\n",
    "\n",
    "In the preprocessing stage, we utilize various technologies and techniques to transform the raw data into a format suitable for machine learning models. We will use the following techniques:\n",
    "\n",
    "- **Numerical Features**\n",
    "Processing and normalizing numerical data features.\n",
    "\n",
    "- **TF-IDF**\n",
    "Using Term Frequency-Inverse Document Frequency (TF-IDF) to convert text data into numerical vectors based on word frequency.\n",
    "\n",
    "- **Word2Vec**\n",
    "Employing Word2Vec to create dense vector representations of words, capturing semantic meanings and relationships.\n",
    "\n",
    "- **BERT**\n",
    "Utilizing Bidirectional Encoder Representations from Transformers (BERT) for creating contextualized word embeddings, enhancing the understanding of word context in sentences.\n",
    "\n",
    "- **SBERT**\n",
    "Using Sentence-BERT (SBERT) to generate embeddings for entire sentences, optimizing for tasks that involve sentence-level semantics.\n",
    "\n",
    "### 3.3.1 Feature Engineering - Numerical Features\n",
    "This stage involves feature creation, relevance analysis, and correlation analysis. From 48 created features 8 remained (relevant feature for predicting scores with Pearson correlation coefficient < 0.9)\n",
    "\n",
    "#### Feature creation\n",
    "We created 48 numerical features, including:\n",
    "\n",
    "1. **Text Analysis Features**: Computes text features such as word count, stopword count, punctuation count, sentence lengths, etc.\n",
    "2. **Ratio Features**: Calculates ratios of different features like distinct words ratio, mistakes ratio, and transitional phrases ratio.\n",
    "3. **Text Statistics Features**: Uses the textstat library to compute readability and complexity metrics such as Flesch reading ease, SMOG index, Coleman-Liau index, and others.\n",
    "\n",
    "#### Relevance Analysis of Numerical Features:\n",
    "To analyze the relevance of these features, we used a combination of statistical tests and visualizations. The methods include:\n",
    "\n",
    "- **F-statistic and p-value**: We used the F-statistic to measure the relationship between each feature and the target variable, with a p-value threshold of 0.05 indicating statistical significance.\n",
    "- **Plot Feature per Classes**: Visualizing the distribution of features across score classes to intuitively assess their relevance.\n",
    "\n",
    "By combining these methods, we identified 20 relevant features and 28 non-relevant features. This comprehensive approach ensures that selected features are consistently relevant across multiple tests, enhancing the accuracy of our scoring prediction model.\n",
    "\n",
    "##### Steps taken\n",
    "1. **F-statistic and p-value Analysis**: Initially with a p-value threshold of 0.05 identified 3 non-relevant features. \n",
    "\n",
    "2. **Categorization Based on F-Statistics**: Established thresholds for High, Medium, and Low relevance categories based on F-statistics, which were later verified through plot analysis. See:\n",
    "\n",
    "<img src=\"f_stat_df.png\" alt=\"f-statistic for Numeric Features\" width=\"400\" height=\"300\"/>\n",
    "*The chart shows the F-statistics for various numeric features. Note: Higher f-statistic values indicate greater relevance and the feature's importance for prediction.*\n",
    "\n",
    "<img src=\"f_statistics_with_relevance_categories.png\" alt=\"Categorization of Numeric Features\" width=\"800\" height=\"600\"/>\n",
    "*The chart shows the F-statistics for various numeric features categorized by their relevance (High, Medium, Low) in predicting essay scores.*\n",
    "\n",
    "3. **Plot Analysis**: Analyzed plots for each category to conclude the relevance of features.\n",
    "\n",
    "##### Plots Examples\n",
    "\n",
    "###### Relevant Feature: Mistakes Dist Ratio\n",
    "The plots below show the distribution of the 'mistakes_dist_ratio' feature across different score classes. The clear trend and separation between score classes indicate that 'mistakes_dist_ratio' is a relevant feature for predicting scores.\n",
    "\n",
    "<img src=\"mistakes_dist_ratio_boxplot.png\" alt=\"Mistakes Dist Ratio Boxplot\" width=\"800\" height=\"600\"/>\n",
    "*The boxplot shows a decreasing trend in 'mistakes_dist_ratio' with higher scores, indicating its relevance.*\n",
    "\n",
    "<img src=\"mistakes_dist_ratio_violinplot.png\" alt=\"Mistakes Dist Ratio Violin Plot\" width=\"800\" height=\"600\"/>\n",
    "*The violin plot highlights the separation between score classes, further confirming the relevance of 'mistakes_dist_ratio'.*\n",
    "\n",
    "###### Non-Relevant Feature: Average Sentence Length\n",
    "The plots below illustrate the distribution of 'avg_sentence_length' across score classes. The lack of clear trends and the presence of significant overlap suggest that 'avg_sentence_length' is not a strong predictor of the score.\n",
    "\n",
    "<img src=\"avg_sentence_length_boxplot.png\" alt=\"Average Sentence Length Boxplot\" width=\"800\" height=\"600\"/>\n",
    "*The boxplot shows overlapping distributions of 'avg_sentence_length' across score classes, indicating low relevance.*\n",
    "\n",
    "<img src=\"avg_sentence_length_violinplot.png\" alt=\"Average Sentence Length Violin Plot\" width=\"800\" height=\"600\"/>\n",
    "*The violin plot confirms the lack of clear trends and significant overlap, further suggesting low relevance.*\n",
    "\n",
    "##### General Observations for Low Relevance Features\n",
    "\n",
    "- **Lack of Clear Trend**: Features do not show a consistent trend across score classes, indicating a weak correlation with the target variable.\n",
    "- **High Variability**: Significant outliers across all score classes diminish the predictive power of these features.\n",
    "- **Overlapping Distributions**: Overlapping in distributions across score classes reduces their relevance in prediction tasks.\n",
    "The lack of clear trends, high variability, and overlapping distributions suggest that low-relevance features are not significant predictors of the score. These features are unlikely to contribute meaningfully to the accuracy and reliability of the scoring prediction model.\n",
    "\n",
    "#### Correlation Analysis of Numerical Features:\n",
    "The purpose of this analysis is to reduce multicollinearity, which can negatively impact the performance of machine learning models.\n",
    "The Pearson correlation coefficient was used for the correlation analysis. \n",
    "Features with a high correlation (greater than 0.9) were excluded to minimize redundancy and improve model performance.\n",
    "From the 20 relevant numerical features, the following 8 features were retained for further use (correlation < 0.9):\n",
    "- 'reading_time'\n",
    "- 'mistakes_dist_ratio'\n",
    "- 'polysyllabcount'\n",
    "- 'sentence_count'\n",
    "- 'difficult_words'\n",
    "- 'comma_count'\n",
    "- 'transitional_phrases_c'\n",
    "- 'text_dist_words_ratio'\n",
    "\n",
    "### 3.3.2 Feature Engineering - TF-IDF\n",
    "**TF-IDF feature extraction**\n",
    "- Starting with max_df=0.99 and min_df=10 in the TfidfVectorizer is a strategic choice to reduce the initial vocabulary size of 54,056 to a more manageable number. \n",
    "  - The max_df=0.99 parameter excludes terms that appear in more than 99% of the documents, removing very common terms that are less informative. \n",
    "  - The min_df=10 parameter excludes terms that appear in fewer than 10 documents, eliminating rare terms that are unlikely to be useful for generalizing across the dataset.\n",
    "\n",
    "**Dimensionality Reduction - PCA**\n",
    "- TF-IDF extracted 6168 features, which is too many for the data set of 13000 rows. It's a good practice to have not more than 10% features. Hence, the dimensionality reduction technique should be applied\n",
    "- Based on the analysis, 1300 components were selected to capture slightly over 85% of the variance. Choosing fewer components would capture significantly less variance and may not be sufficient for effective data representation. See histogram and plot for visibility\n",
    "\n",
    "<img src=\"explained_variance_ratio_histogram.png\" alt=\"Explained Variance ratio Histogram\" width=\"800\" height=\"600\"/>\n",
    "*The histogram shows that the first few principal components explain a significant portion of the variance, while the contribution of each subsequent component rapidly decreases.*\n",
    "\n",
    "<img src=\"cumulative_explained_variance_ratio_plot.png\" alt=\"Cumulative Explained Variance Ratio Plot\" width=\"800\" height=\"600\"/>\n",
    "*The plot shows that 1300 components are required to capture slightly over 85% of the total variance. The curve rises steeply initially and then starts to flatten out, indicating diminishing returns for each additional component.*\n",
    "\n",
    "### 3.3.3 Feature Engineering - Combine Numerical and TF-IDF features\n",
    "In later stages, we can experiment with models on different sets of features (e.g., numerical, TF-IDF, or combined features). Here, we focus on preparing combined numerical and TF-IDF features.\n",
    "\n",
    "- **Numerical Features**: Given the range of TF-IDF values (approximately -0.52 to 0.69) numerical features were scaled to ensure each contributes equally to the analysis. Scaling is necessary because numerical features have different ranges and magnitudes, and it prevents features with larger ranges from dominating the results. \n",
    "- **TF-IDF Features**: These vectors are already normalized, representing term frequencies normalized by document frequencies. Scaling TF-IDF features again could distort their inherent meaning.\n",
    "\n",
    "### 3.3.4 Feature Engineering - Word2Vect\n",
    "Using the Word2Vec technique for essay scoring tasks can capture the semantic relationships and contextual meanings of words, providing richer and more nuanced text representations that can improve the accuracy of scoring based on content and coherence. \n",
    "\n",
    "Exploratory Data Analysis (EDA) was performed to understand the dataset characteristics such as vocabulary size, minimum text length, maximum text length, average text length, and average sentence length. Based on these insights, the following parameters were chosen for the Word2Vec model:\n",
    "\n",
    "- **vector_size=300:** This larger size helps capture more semantic nuances, which is beneficial for a diverse and rich vocabulary.\n",
    "- **window=5:** A window size of 5 is sufficient to capture the context within the average sentence length of approximately 20 words.\n",
    "- **min_count=10:** Filters out infrequent words, reducing noise and focusing on more common and likely more informative words.\n",
    "- **workers=32:** Utilizes all 32 CPU cores to speed up the training process, making efficient use of the available computational resources.\n",
    "\n",
    "### 3.3.4 Feature Engineering - BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) provides contextualized word embeddings that capture the meaning of words based on their context within a sentence. This is particularly beneficial for tasks like essay scoring, where understanding the nuanced meaning of sentences and their coherence is crucial. \n",
    "\n",
    "**Exploratory Data Analysis**\n",
    "Texts longer than 512 tokens are truncated. The length of texts was explored\n",
    "\n",
    "- Text Length Statistics by Score\n",
    "| Score | Count  | Mean       | Std Dev    | Min  | 25%  | 50%  | 75%   | Max   |\n",
    "|-------|--------|------------|------------|------|------|------|-------|-------|\n",
    "| 1     | 1001.0 | 319.931069 | 127.523991 | 168.0| 229.0| 282.0| 376.00| 1109.0|\n",
    "| 2     | 3778.0 | 308.344362 | 115.177906 | 164.0| 232.0| 280.0| 350.75| 1824.0|\n",
    "| 3     | 5022.0 | 422.313222 | 120.774191 | 182.0| 338.0| 403.0| 485.00| 1361.0|\n",
    "| 4     | 3141.0 | 566.596307 | 130.210442 | 266.0| 478.0| 548.0| 632.00| 1608.0|\n",
    "| 5     | 776.0  | 752.271907 | 160.134917 | 427.0| 638.0| 730.0| 847.25| 1617.0|\n",
    "| 6     | 125.0  | 908.688000 | 179.384675 | 611.0| 785.0| 890.0| 982.00| 1582.0|\n",
    "\n",
    "- Maximum token length: 1824\r",
    "- \n",
    "Percentage of texts within BERT's token limit: 70.50%\n",
    "\n",
    "<img src=\"text_length_distribution_with_scores.png\" alt=\"Text Length Distribution\" width=\"800\" height=\"600\"/>\n",
    "*We can see that texts longer than 512 tokens (all with score=6 and majority with score=5) will lose the content beyond this limit, potentially leading to an incomplete representation of the text's quality and coherence, which are crucial for essay scoring.*\n",
    "\n",
    "\n",
    "**Alternative Approaches**:\n",
    "- **Longer Context Models:** Consider using models designed for longer contexts, such as Longformer or BigBird, which can handle longer sequences without truncation.\n",
    "- **Chunking:** Split longer texts into chunks of 512 tokens and aggregate their embeddings, although this might not fully capture the global context and coherence of the entire text.\n",
    "\n",
    "However, let's proceed with BERT to see its results first, and consider alternative approaches if necessary.\n",
    "\n",
    "**BERT Embedding Process**\n",
    "- The `full_text` column is converted to lowercase to ensure consistency with the bert-base-uncased model, which is used to capture the full context and semantics of the text. \n",
    "- Texts longer than 512 tokens are truncated, and all texts are padded to a maximum of 512 tokens.\n",
    "- The get_bert_embedding function extracts 768-dimensional embeddings from the [CLS] token for each text, capturing rich contextual information for essay scoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3032471c-5a11-495b-8152-770543f4103d",
   "metadata": {},
   "source": [
    "## 4. Experiments\n",
    "## 4.1. Experiment 1 (train models on the initial 17k data set) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f92aa7-ab45-486f-9b97-302a0c530554",
   "metadata": {},
   "source": [
    "This experiment used 80% of the initial `train.csv` dataset (with 20% left unseen), splitting it again into 80/20 for train and test sets (stratified by score). \n",
    "\n",
    "Performance on both the train (11074 rows) and test (2769 rows sets was analyzed to evaluate the performance of various regression models on different feature sets: \n",
    "- numerical features,\n",
    "- TF-IDF features,\n",
    "- combined numerical and TF-IDF features,\n",
    "- Word2Vec features,\n",
    "- BERT embeddings (gave up due to long run time).\n",
    "\n",
    "The models include\n",
    "- Linear Regression, Random Forest Regressor, AdaBoost Regressor, CatBoost Regressor, XGBoost Regressor, and LightGBM Regressor.\n",
    "Detailed results are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d856d-2576-42fa-8c88-6eb78050bcd8",
   "metadata": {},
   "source": [
    "### 4.1.1 Experiment Summary\n",
    "\n",
    "This experiment used 80% of the initial train.csv dataset (20% is still unseen), splitting it again into 80/20. The performance of the seen data of both train_split and test_split sets was analyzed to evaluate the performance of various regression models on different feature sets: numerical features, TF-IDF features, combined numerical and TF-IDF features, Word2Vec features, and BERT embeddings. The models tested include Linear Regression, Random Forest Regressor, AdaBoost Regressor, CatBoost Regressor, XGBoost Regressor, and LightGBM Regressor. Detailed results are shown below.\n",
    "\n",
    "### 4.1.2 Results\n",
    "\n",
    "#### 4.1.2.1 Numerical Features Only\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Linear Regression      | 0.438766          | 0.661431         |\n",
    "| Random Forest Regressor| 0.719541          | 0.716102         |\n",
    "| AdaBoost Regressor     | 0.716374          | 0.707997         |\n",
    "| CatBoost Regressor     | 0.738748          | 0.714213         |\n",
    "| XGBoost Regressor      | 0.760924          | 0.725878         |\n",
    "| LightGBM Regressor     | 0.768217          | 0.705879         |\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| CatBoost Classifier    | 0.738946          | 0.697195         |\n",
    "| XGBoost Classifier     | 0.719882          | 0.690799         |\n",
    "| LightGBM Classifier    | 0.910527          | 0.679942         |\n",
    "| Random Forest Classifier| 0.674805         | 0.670565         |\n",
    "| Logistic Regression    | 0.671035          | 0.670444         |\n",
    "| AdaBoost Classifier    | 0.339885          | 0.335948         |\n",
    "\n",
    "#### 4.1.2.2 TF-IDF Features Only\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Linear Regression      | 0.650407          | 0.485852         |\n",
    "| CatBoost Regressor     | 0.725470          | 0.583873         |\n",
    "| XGBoost Regressor      | 0.848442          | 0.520738         |\n",
    "| LightGBM Regressor     | 0.951018          | 0.533020         |\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Logistic Regression    | 0.663757          | 0.578316         |\n",
    "| LightGBM Classifier    | 1.000000          | 0.513538         |\n",
    "| CatBoost Classifier    | 0.788927          | 0.510062         |\n",
    "| XGBoost Classifier     | 0.993334          | 0.506426         |\n",
    "\n",
    "#### 4.1.2.3 Combined Numerical and TF-IDF Features\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Linear Regression      | 0.697232          | 0.672009         |\n",
    "| CatBoost Regressor     | 0.870019          | 0.737538         |\n",
    "| XGBoost Regressor      | 0.942193          | 0.752054         |\n",
    "| LightGBM Regressor     | 0.970799          | 0.738065         |\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| CatBoost Classifier    | 0.847677          | 0.728796         |\n",
    "| XGBoost Classifier     | 0.995395          | 0.723560         |\n",
    "| LightGBM Classifier    | 1.000000          | 0.718091         |\n",
    "\n",
    "#### 4.1.2.4 Word2Vec Features Only\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Linear Regression      | 0.355899          | 0.351317         |\n",
    "| CatBoost Regressor     | 0.523580          | 0.373310         |\n",
    "| XGBoost Regressor      | 0.673253          | 0.461169         |\n",
    "| LightGBM Regressor     | 0.781056          | 0.364984         |\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| XGBoost Classifier     | 0.887020          | 0.567956         |\n",
    "| LightGBM Classifier    | 0.998553          | 0.563399         |\n",
    "| CatBoost Classifier    | 0.701877          | 0.557159         |\n",
    "| AdaBoost Classifier    | 0.426707          | 0.391434         |\n",
    "| Random Forest Classifier| 0.267254         | 0.250111         |\n",
    "\n",
    "*Word2Vec with a Simple NN using Tensor Library*\n",
    "\n",
    "    - QWK Score on Train Set: 0.6412155326583289\n",
    "    - QWK Score on Test Set: 0.5535504981689063\n",
    "\n",
    "#### 4.1.2.5 BERT\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| CatBoost Classifier    | 0.823684          | 0.679635         |\n",
    "| XGBoost Classifier     | 0.975234          | 0.690236         |\n",
    "| LightGBM Classifier    | 1.000000          | 0.683610         |\n",
    "\n",
    "| Model                  | QWK Score (Train) | QWK Score (Test) |\n",
    "|------------------------|-------------------|------------------|\n",
    "| Linear Regression      | 0.557390          | 0.506605         |\n",
    "| CatBoost Regressor     | 0.756849          | 0.591912         |\n",
    "| XGBoost Regressor      | 0.796409          | 0.596577         |\n",
    "| LightGBM Regressor     | 0.932279          | 0.568143         |\n",
    "\n",
    "*BERT with a Simple NN using TensorFlow*\n",
    "    - QWK Score on Train Set: 0.7216489652760272\n",
    "    - QWK Score on Test Set: 0.5787502629917947\n",
    "\n",
    "*Note: AdaBoost and Random Forest*\n",
    "\n",
    "- **Resource Usage:** The laptop was hanging when running AdaBoost and Random Forest, so these models were abandoned due to excessive resource consumption.\n",
    "\n",
    "### 4.1.3 Summary of Results and Recommendations\n",
    "\n",
    "1. **Proceed with Combined Numerical and TF-IDF Features:**\n",
    "   - **Reason:** <span style=\"color:red\">The combination of numerical and TF-IDF features yields the best performance across models, particularly with XGBoost Regressor (QWK Score of 0.752054 on the test set).</span>  This indicates that the combination of different types of features provides a more comprehensive representation of the data, leading to better model performance.\n",
    "   - Word2Vec might have performed worse than Numerical+TF-IDF because the context captured by Word2Vec embeddings may not have been sufficient to fully represent the nuanced criteria used by teachers to score essays, such as coherence, structure, and specific content, which are better captured by the combined numerical and TF-IDF features.\n",
    "\n",
    "2. **Undersampled Scores Issue:**\n",
    "   - **Observation:** Scores 1, 5, and 6, which are undersampled, are sometimes not predicted at all, even with stratified splitting. This indicates a need for additional training data to better represent these scores.\n",
    "   - **Examples:**\n",
    "     - Distinct predicted values on the test set for CatBoost Classifier: `[1, 2, 3, 4, 5]`\n",
    "     - Distinct predicted values on the test set for Logistic Regression: `[2, 3, 4, 5]`\n",
    "\n",
    "3. **Note on Train and Test Score Differences:**\n",
    "   - **Reason for Differences:** The significant differences between training and test scores, especially with TF-IDF and Word2Vec features, suggest overfitting. The models are performing well on the training data but failing to generalize to the test data.\n",
    "   - **Overfitting Indicators:** High training scores coupled with much lower test scores are classic signs of overfitting. This occurs when a model learns the training data too well, including noise and outliers, but fails to capture the underlying patterns applicable to the test data.\n",
    "\n",
    "4. **Next Steps:**\n",
    "   - **Address Overfitting:** Implement regularization techniques, use cross-validation, apply hyperparameter tuning to improve model generalizability.\n",
    "   - **Further Feature Engineering:** Explore additional features or combinations that may improve model performance.\n",
    "   - **Feature Selection for TF-IDF:** Consider reducing the number of TF-IDF features as part of hyperparameter tuning. This could help mitigate overfitting and improve model performance. Try different values for `max_features` or adjust `max_df` and `min_df` parameters to reduce the feature space.\n",
    "   - **Model Ensemble:** Consider ensemble methods like stacking or blending to leverage the strengths of multiple models. However, these methods may involve increased computational cost, complexity, and longer training times. Therefore, it might be best to forgo this approach.\n",
    "\n",
    "5. **Models to Proceed with:**\n",
    "   - **Focus on High-Performing Models:** XGBoost Regressor and CatBoost Regressor are strong candidates due to their high QWK scores on both numerical and combined features <span style=\"color:red\">CatBoost Regressor might generalize better to unseen data given its slightly smaller gap between train and test scores.</span> .\n",
    "   - **Balanced Approach:** While we can proceed with the best-performing models, it's still beneficial to try different models with proper cross-validation and regularization to ensure robustness and generalizability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc7b39-45ef-43dc-bc95-132ed815bb82",
   "metadata": {},
   "source": [
    "## 4.2. Experiment 2 (train models on expanded data set by esaays with underpresented scores 1,5,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc158b2-7306-4e83-b572-7c97ff355c6d",
   "metadata": {},
   "source": [
    "Experiment 2 is built upon Experiment 1 by improving the dataset. The original dataset of 17,000 rows has been expanded by incorporating additional essays sourced from the Internet, particularly focusing on underrepresented scores of 1, 5, and 6. Consequently, all preprocessing steps, including cleaning, transforming, feature engineering, and model training, have been re-executed. Below, we outline the key differences from Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c1d2b-2fb7-4167-bb0f-1ed195971529",
   "metadata": {},
   "source": [
    "### 4.2.1 Comparison of initial data, preprocessing and model training for Experiment 1 and Experiment 2\n",
    "\n",
    "| Aspect                        | Experiment 1                                                            | Experiment 2                                                            |\n",
    "|-------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
    "| **Initial ED**               |                                                                         |                                                                         |\n",
    "| Dataset Size                  | 17,307 entries, 3 columns                                               | 22,567 entries, 3 columns                                               |\n",
    "| Score Distribution            | Imbalanced, scores 1, 5, and 6 underrepresented                         | Imbalanced, scores 1, 5, and 6 underrepresented                         |\n",
    "| Score Distribution Plot       |                     | see the plot below\n",
    "|\n",
    "| **Duplicates Handling**       | Duplicates deleted                                                      | Duplicates retained                                                     |\n",
    "| **Split Data**                |                                                                         |                                                                         |\n",
    "| Training Set Size             | 13,845 entries, 3 columns                                               | 18,053 entries, 3 columns                                               |\n",
    "| Testing Set Size              | 3,462 entries, 3 columns                                                | 4,514 entries, 3 columns                                                |\n",
    "| Split Method                  | Stratified sampling                                                     | Stratified sampling                                                     |\n",
    "| **Model Training**            |                                                                         |                                                                         |\n",
    "| Models Trained                | Linear Regression, Random Forest Regressor, AdaBoost Regressor, CatBoost Regressor, XGBoost Regressor, LightGBM Regressor, BERT, Word2Vec | the same except BERT (BERT skipped due to high resource consumption and poor results) |\n",
    "| Features Used                 | Numerical Features Only, TF-IDF Features Only, Combined Numerical and TF-IDF Features, Word2Vec Features Only | the same |\n",
    "\n",
    "<img src=\"score_distribution_plot_exp_2.png\" alt=\"Score Distribution\" width=\"1200\" height=\"900\"/>\n",
    "\n",
    "*The plot on the left shows the distribution of scores in the training dataset for Experiment 1, highlighting the imbalance, while the plot on the right compares the distribution of scores in the original dataset (df1) and the combined dataset (df_combined) for Experiment 2, demonstrating the improved representation of underrepresented scores 1, 5, and 6.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37076d73-0148-49e9-9efb-62c6a90b7858",
   "metadata": {},
   "source": [
    "### 4.2.2 Comparison of the results from Experiment 1 and Experiment 2\n",
    "\n",
    "#### 4.2.2.1 Numerical Features Only\n",
    "\n",
    "| Model                   | Experiment 1 Train | Experiment 1 Test | Experiment 2 Train | Experiment 2 Test |\n",
    "|-------------------------|--------------------|-------------------|--------------------|-------------------|\n",
    "| Linear Regression       | 0.438766           | 0.661431          | 0.438766           | 0.661431          |\n",
    "| Random Forest Regressor | 0.719541           | 0.716102          | 0.719541           | 0.716102          |\n",
    "| AdaBoost Regressor      | 0.716374           | 0.707997          | 0.716374           | 0.707997          |\n",
    "| CatBoost Regressor      | 0.738748           | 0.714213          | 0.738748           | 0.714213          |\n",
    "| XGBoost Regressor       | 0.760924           | 0.725878          | 0.760924           | 0.725878          |\n",
    "| LightGBM Regressor      | 0.768217           | 0.705879          | 0.768217           | 0.705879          |\n",
    "| CatBoost Classifier     | 0.738946           | 0.697195          | 0.840578           | 0.833498          |\n",
    "| XGBoost Classifier      | 0.719882           | 0.690799          | 0.836412           | 0.828504          |\n",
    "| LightGBM Classifier     | 0.910527           | 0.679942          | 0.942789           | 0.814261          |\n",
    "| Random Forest Classifier| 0.674805           | 0.670565          | 0.808717           | 0.810941          |\n",
    "| Logistic Regression     | 0.671035           | 0.670444          | 0.805838           | 0.812265          |\n",
    "| AdaBoost Classifier     | 0.339885           | 0.335948          | 0.633158           | 0.634854          |\n",
    "\n",
    "#### 4.2.2.2 TF-IDF Features Only\n",
    "\n",
    "| Model                   | Experiment 1 Train | Experiment 1 Test | Experiment 2 Train | Experiment 2 Test |\n",
    "|-------------------------|--------------------|-------------------|--------------------|-------------------|\n",
    "| Linear Regression       | 0.650407           | 0.485852          | 0.650407           | 0.485852          |\n",
    "| CatBoost Regressor      | 0.725470           | 0.583873          | 0.725470           | 0.583873          |\n",
    "| XGBoost Regressor       | 0.848442           | 0.520738          | 0.848442           | 0.520738          |\n",
    "| LightGBM Regressor      | 0.951018           | 0.533020          | 0.951018           | 0.533020          |\n",
    "| Logistic Regression     | 0.663757           | 0.578316          | 0.793493           | 0.739610          |\n",
    "| LightGBM Classifier     | 1.000000           | 0.513538          | 1.000000           | 0.714014          |\n",
    "| CatBoost Classifier     | 0.788927           | 0.510062          | 0.833477           | 0.696338          |\n",
    "| XGBoost Classifier      | 0.993334           | 0.506426          | 0.993676           | 0.712289          |\n",
    "\n",
    "#### 4.2.2.3 Combined Numerical and TF-IDF Features\n",
    "\n",
    "| Model                   | Experiment 1 Train | Experiment 1 Test | Experiment 2 Train   | Experiment 2 Test   |\n",
    "|-------------------------|--------------------|-------------------|----------------------|---------------------|\n",
    "| Linear Regression       | 0.697232           | 0.672009          | 0.758918             | 0.781659            |\n",
    "| <span style=\"color:red\">CatBoost Regressor</span>      | 0.870019           | 0.737538          | <span style=\"color:red\">0.904795</span> | <span style=\"color:red\">0.861679</span> |\n",
    "| XGBoost Regressor       | 0.942193           | 0.752054          | 0.949607             | 0.864658            |\n",
    "| LightGBM Regressor      | 0.970799           | 0.738065          | 0.961129             | 0.850613            |\n",
    "| CatBoost Classifier     | 0.847677           | 0.728796          | 0.902166             | 0.851082            |\n",
    "| XGBoost Classifier      | 0.995395           | 0.723560          | 0.995851             | 0.854455            |\n",
    "| LightGBM Classifier     | 1.000000           | 0.718091          | 1.000000             | 0.849610            |\n",
    "\n",
    "\n",
    "\n",
    "#### 4.2.2.4 Word2Vec Features Only\n",
    "\n",
    "| Model                   | Experiment 1 Train | Experiment 1 Test | Experiment 2 Train | Experiment 2 Test |\n",
    "|-------------------------|--------------------|-------------------|--------------------|-------------------|\n",
    "| Linear Regression       | 0.355899           | 0.351317          | 0.355899           | 0.351317          |\n",
    "| CatBoost Regressor      | 0.523580           | 0.373310          | 0.523580           | 0.373310          |\n",
    "| XGBoost Regressor       | 0.673253           | 0.461169          | 0.673253           | 0.461169          |\n",
    "| LightGBM Regressor      | 0.781056           | 0.364984          | 0.781056           | 0.364984          |\n",
    "| XGBoost Classifier      | 0.887020           | 0.567956          | 0.887020           | 0.567956          |\n",
    "| LightGBM Classifier     | 0.998553           | 0.563399          | 0.998553           | 0.563399          |\n",
    "| CatBoost Classifier     | 0.701877           | 0.557159          | 0.701877           | 0.557159          |\n",
    "| AdaBoost Classifier     | 0.426707           | 0.391434          | 0.426707           | 0.391434          |\n",
    "| Random Forest Classifier| 0.267254           | 0.250111          | 0.267254           | 0.250111          |\n",
    "\n",
    "*Word2Vec with a simple feedforward neural network using TensorFlow Library*\n",
    "\n",
    "| Metric                  | Experiment 1       | Experiment 2       |\n",
    "|-------------------------|--------------------|--------------------|\n",
    "| QWK Score on Train Set  | 0.6412155326583289 | 0.6412155326583289 |\n",
    "| QWK Score on Test Set   | 0.5535504981689063 | 0.5535504981689063 |\n",
    "\n",
    "#### 4.2.2.6 BERT\n",
    "\n",
    "| Model                   | Experiment 1 Train | Experiment 1 Test  | Experiment 2 Train | Experiment 2 Test |\n",
    "|-------------------------|--------------------|--------------------|--------------------|-------------------|\n",
    "| CatBoost Classifier     | 0.823684           | 0.679635           | skipped            | skipped           |\n",
    "| XGBoost Classifier      | 0.975234           | 0.690236           | skipped            | skipped           |\n",
    "| LightGBM Classifier     | 1.000000           | 0.683610           | skipped            | skipped           |\n",
    "| Linear Regression       | 0.557390           | 0.506605           | skipped            | skipped           |\n",
    "| CatBoost Regressor      | 0.756849           | 0.591912           | skipped            | skipped           |\n",
    "| XGBoost Regressor       | 0.796409           | 0.596577           | skipped            | skipped           |\n",
    "| LightGBM Regressor      | 0.932279           | 0.568143           | skipped            | skipped           |\n",
    "\n",
    "*BERT with a simple feedforward neural network using TensorFlow Library*\n",
    "\n",
    "| Metric                  | Experiment 1       | Experiment 2       |\n",
    "|-------------------------|--------------------|--------------------|\n",
    "| QWK Score on Train Set  | 0.7216489652760272 | skipped            |\n",
    "| QWK Score on Test Set   | 0.5787502629917947 | skipped            |\n",
    "\n",
    "### 4.2.3 Summary of Results and Recommendations\n",
    "\n",
    "**Results Improvement**: Experiment 2 shows improved QWK scores across most models compared to Experiment 1, particularly for combined numerical and TF-IDF features.\n",
    "\n",
    "**Train-Test Difference**: The difference between train and test QWK scores has generally reduced in Experiment 2, indicating better generalization.\n",
    "\n",
    "**Model to Proceed With**: The **CatBoost Regressor** with combined numerical and TF-IDF features demonstrates the best balance of high QWK scores and reduced overfitting.\n",
    "\n",
    "**Next Steps**:\n",
    "1. **TF-IDF Dimensionality Reduction**: Consider reducing dimensionality while retaining essential information by applying PCA. This could help mitigate overfitting and improve model performance. Try different values for `max_features` or adjust `max_df` and `min_df` parameters to reduce the feature space.\n",
    "2. **Hyperparameter Tuning**: Perform grid search or random search to fine-tune the CatBoost Regressor parameters.\n",
    "3. **Further Address Overfitting**: Implement regularization techniques and use cross-validation to improve model generalizability.\n",
    "4. **Cross-Validation**: Implement cross-validation to ensure robustness and generalizability of the model.\n",
    "5. **Model Evaluation**: Continuously evaluate the model on a separate validation set to monitor performance and adjust as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982fe50-67cc-47a4-9e36-4b4b5f6dcb22",
   "metadata": {},
   "source": [
    "## 5. Dimentionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07cae2-a149-4240-bf2e-6a9a98ac6a39",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) was applied to reduce the number of features from 1300 to 1000, 700, and 500, explaining 80%, 75%, and 70% of the variance, respectively. \n",
    "See Notebooks 15.1, 15.2, 15.3, 16, and 17 in Experiment 2 for detailed steps and analysis.\n",
    "\n",
    "The following results were achieved using CatBoostRegressor (iterations=600, depth=4, learning_rate=0.1):\n",
    "\n",
    "| Dataset                              | Count of PCA Components | Count of Features | QWK Score (Train) | QWK Score (Test) |\n",
    "|--------------------------------------|-------------------------|-------------------|-------------------|------------------|\n",
    "| combined_features_exp_2.csv          | 1300                    | 1308              | 0.904795          | 0.861679         |\n",
    "| combined_features_exp_2_pca_1000.csv | 1000                    | 1008              | 0.906795          | 0.862143         |\n",
    "| combined_features_exp_2_pca_700.csv  | 700                     | 708               | 0.906436          | 0.862702         |\n",
    "| combined_features_exp_2_pca_500.csv  | 500                     | 508               | 0.902056          | 0.867482         |\n",
    "\n",
    "Despite the improvements, there is still a difference between the train and test set scores, indicating that some overfitting persists.                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afde56-dafe-4b02-a810-f6450ce4d39d",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea526a4-61dd-46ac-9821-383e83e3b60a",
   "metadata": {},
   "source": [
    "After performing Randomized Search Cross-Validation with CatBoostRegressor on multiple datasets, the best hyperparameters were identified based on the Quadratic Weighted Kappa (QWK) score. \n",
    "\n",
    "**Datasets tried:**\n",
    "- 'combined_features_exp_2.csv'\n",
    "- 'combined_features_exp_2_pca_1000.csv'\n",
    "- 'combined_features_exp_2_pca_700.csv'\n",
    "- 'combined_features_exp_2_pca_500.csv'\n",
    "\n",
    "**Parameters tried:**\n",
    "- iterations: 300, 500, 600, 1000\n",
    "- depth: 4, 6, 8\n",
    "- learning_rate: 0.01, 0.05, 0.1\n",
    "- l2_leaf_reg: 1, 3, 5, 7, 9\n",
    "\n",
    "**The following parameters were chosen:**\n",
    "\n",
    "- **Dataset:** combined_features_exp_2_pca_500.csv (508 features: 8 numerical and 500 TF-IDF, PCA explaining 70% of variance)\n",
    "\n",
    "- **Params:** `{'learning_rate': 0.01, 'l2_leaf_reg': 7, 'iterations': 500, 'depth': 4}`\n",
    "\n",
    "**Reasons for this choice:**\n",
    "\n",
    "1. **Balanced Performance:** \n",
    "   - The selected model has a good balance between the train and test QWK scores, indicating it is not significantly overfitting. \n",
    "   \n",
    "| Metric                | QWK Score         | Standard Deviation |\n",
    "|-----------------------|-------------------|--------------------|\n",
    "| Mean CV QWK Score     | 0.832616553       | 0.003274856        |\n",
    "| Train QWK Score       | <span style=\"color:red\">0.837511829</span>       | 0.001020374        |\n",
    "| Test QWK Score        | <span style=\"color:red\">0.836941581</span>       | 0.004895276        |\n",
    "\n",
    "2. **Generalization:** \n",
    "   - Among models with similar performance, we prefer those with lower iterations because they are less likely to overfit and generalize better to unseen data.\n",
    "\n",
    "3. **Performance Improvement:** \n",
    "   - The chosen option has quite good performance, which is better than the results achieved in Experiment 1 (train 0.870, <span style=\"color:red\">test 0.737</span>)\n",
    "\n",
    "By choosing these parameters, we aim to achieve a model that performs well on new data while maintaining stability and consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ad748-26d4-40ac-a43d-6bd051ad8d0c",
   "metadata": {},
   "source": [
    "## 7. Predict on Unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2552d-0eb2-4654-9ad0-38ae4c2a3548",
   "metadata": {},
   "source": [
    "The result was predicted on the unseen data set (test_split.csv) which we put aside at the very beginning (it contains 20% of the oversampled data set). Data was transformed in the same way as the train_split data set. Pre-trained scaler, TF-IDF vectorizer, PCA, and the model were applied.\n",
    "\n",
    "**Results:**\n",
    "- **Quadratic Weighted Kappa Score:** 0.8359\n",
    "\n",
    "- **Confusion Matrix:**\n",
    "\n",
    "|   | 1   | 2    | 3    | 4    | 5   | 6  |\n",
    "|---|-----|------|------|------|-----|----|\n",
    "| 1 | 1   | 204  | 44   | 18   | 0   | 0  |\n",
    "| 2 | 0   | 1055 | 241  | 44   | 2   | 0  |\n",
    "| 3 | 0   | 258  | 706  | 270  | 22  | 0  |\n",
    "| 4 | 0   | 4    | 188  | 461  | 132 | 0  |\n",
    "| 5 | 0   | 0    | 1    | 90   | 591 | 4  |\n",
    "| 6 | 0   | 0    | 0    | 0    | 160 | 18 |\n",
    "\n",
    "- **Detailed Analysis:**\n",
    "    \n",
    "| Actual | Predicted | Count |\n",
    "|--------|-----------|-------|\n",
    "| 1      | 2         | 204   |\n",
    "| 1      | 3         | 44    |\n",
    "| 1      | 4         | 18    |\n",
    "| 1      | 5         | 0     |\n",
    "| 1      | 6         | 0     |\n",
    "| 2      | 1         | 0     |\n",
    "| 2      | 3         | 241   |\n",
    "| 2      | 4         | 44    |\n",
    "| 2      | 5         | 2     |\n",
    "| 2      | 6         | 0     |\n",
    "| 3      | 1         | 0     |\n",
    "| 3      | 2         | 258   |\n",
    "| 3      | 4         | 270   |\n",
    "| 3      | 5         | 22    |\n",
    "| 3      | 6         | 0     |\n",
    "| 4      | 1         | 0     |\n",
    "| 4      | 2         | 4     |\n",
    "| 4      | 3         | 188   |\n",
    "| 4      | 5         | 132   |\n",
    "| 4      | 6         | 0     |\n",
    "| 5      | 1         | 0     |\n",
    "| 5      | 2         | 0     |\n",
    "| 5      | 3         | 1     |\n",
    "| 5      | 4         | 90    |\n",
    "| 5      | 6         | 4     |\n",
    "| 6      | 1         | 0     |\n",
    "| 6      | 2         | 0     |\n",
    "| 6      | 3         | 0     |\n",
    "| 6      | 4         | 0     |\n",
    "| 6      | 5         | 160   |\n",
    "\n",
    "The detailed analysis provides insights into specific misclassifications. Notable points include:\n",
    "\n",
    "- The majority of instances are correctly classified, especially for classes 2 and 3.\n",
    "- Some instances of class 1 are misclassified as class 2 (204) and class 3 (44).\n",
    "- Some instances of class 3 are misclassified as class 2 (258) and class 4 (270).\n",
    "- Misclassifications are less frequent for higher classes (5 and 6), but there are still some notable errors, such as class 6 being predicted as class 5 (160 times).\n",
    "\n",
    "**Conclusion:**\n",
    "<span style=\"color:red\">The QWK score on the unseen data set is 0.8359</span>, which is consistent with the QWK scores observed during the training and validation process. This indicates that the model has generalized well to the unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26194b5b-ffd3-4e53-bf11-3bb61c3be680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
