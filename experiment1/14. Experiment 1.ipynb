{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ac3d02-9285-4427-aaf6-74ee077f5c91",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Summary of Steps](#Summary-of-Steps)\n",
    "- [Imports](#Imports)\n",
    "- [Classification Models](#Classification-Models)\n",
    "  - [Numerical Features](#Numerical-Features)\n",
    "  - [TF-IDF Features](#TF-IDF-Features)\n",
    "  - [Combined Numerical and TF IDF Features](#Combined-Numerical-and-TF-IDF-Features)\n",
    "- [Regression Models](#Regression-Models)\n",
    "  - [Numerical Features](#Numerical-Features)\n",
    "  - [TF-IDF Features](#TF-IDF-Features)\n",
    "  - [Combined Numerical and TF-IDF Features](#Combined-Numerical-and-TF-IDF-Features)\n",
    "- [Word2Vec Features](#Word2Vec-Features)\n",
    "  - [Option 1: Traditional Machine Learning Classification Models](#Option-1:-Traditional-Machine-Learning-Classification-Models)\n",
    "  - [Option 2: Traditional Machine Learning Regression Models](#Option-2:-Traditional-Machine-Learning-Regression-Models)\n",
    "  - [Option 3: Using Neural Networks](#Option-3:-Using-Neural-Networks)\n",
    "- [BERT](#BERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90e6d0-602c-4e2e-96bc-d175b1d24625",
   "metadata": {},
   "source": [
    "## Summary of Steps\n",
    "\n",
    "1. **Load Data:** Load the `numeric_features_added_v1.csv`.\n",
    "2. **Define Features:** Define the numerical features and extract them from the DataFrame.\n",
    "3. **Split Data:** Split the data into training and test sets.\n",
    "4. **Train Model:** Train the `CatBoostRegressor` on the training data.\n",
    "5. **Predict:** Make predictions on the test data.\n",
    "6. **Discretize Predictions:** Discretize both the predictions and the actual test labels.\n",
    "7. **Evaluate:** Compute the Quadratic Weighted Kappa Score to evaluate the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b43fa-db53-4380-8cca-5e5179124e1a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e49157c-df91-428e-bfb6-8f7208fe3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b37d2b8-4acc-42b9-bc29-723feea011bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df05a14b-4f34-42b7-b2a2-f3dbccb5091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: LightGBM was commented after execution to avoid heavy logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eeb38-3448-4a47-9b7b-808d26337a13",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6130c6-360b-4204-90ff-37baa192ff85",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "385e6119-cffa-42d3-b103-f3faf13b2d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "2    4017\n",
      "1    3022\n",
      "3    2513\n",
      "0     801\n",
      "4     621\n",
      "5     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "2    1005\n",
      "1     756\n",
      "3     628\n",
      "0     200\n",
      "4     155\n",
      "5      25\n",
      "Name: count, dtype: int64\n",
      "Working on Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for Logistic Regression: 8.929550170898438 seconds\n",
      "Distinct predicted values on training set for Logistic Regression: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for Logistic Regression: [2 3 4 5]\n",
      "Working on Random Forest Classifier...\n",
      "Elapsed time for Random Forest Classifier: 15.649439573287964 seconds\n",
      "Distinct predicted values on training set for Random Forest Classifier: [2 3 4 5]\n",
      "Distinct predicted values on test set for Random Forest Classifier: [2 3 4 5]\n",
      "Working on AdaBoost Classifier...\n",
      "Elapsed time for AdaBoost Classifier: 4.318726539611816 seconds\n",
      "Distinct predicted values on training set for AdaBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for AdaBoost Classifier: [1 2 3 4 5 6]\n",
      "Working on CatBoost Classifier...\n",
      "Elapsed time for CatBoost Classifier: 6.367633104324341 seconds\n",
      "Distinct predicted values on training set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Working on XGBoost Classifier...\n",
      "Elapsed time for XGBoost Classifier: 10.632531642913818 seconds\n",
      "Distinct predicted values on training set for XGBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for XGBoost Classifier: [1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test datasets\n",
    "df = pd.read_csv('numeric_features_added_v1.csv')\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = [\n",
    "    'reading_time',\n",
    "    'mistakes_dist_ratio',\n",
    "    'polysyllabcount',\n",
    "    'sentence_count',\n",
    "    'difficult_words',\n",
    "    'comma_count',\n",
    "    'transitional_phrases_c',\n",
    "    'text_dist_words_ratio'\n",
    "]\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = df[numerical_features]\n",
    "y = df['score'] - 1  # Adjust class labels to start from 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest Classifier': RandomForestClassifier(n_estimators=600, max_depth=4, random_state=42),\n",
    "    'AdaBoost Classifier': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost Classifier': CatBoostClassifier(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Classifier': XGBClassifier(objective='multi:softmax', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Classifier': LGBMClassifier(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on training set for {name}: {np.unique(y_train_pred)}\")\n",
    "    kappa_train_score = cohen_kappa_score(y_train + 1, y_train_pred, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on test set for {name}: {np.unique(y_test_pred)}\")\n",
    "    kappa_test_score = cohen_kappa_score(y_test + 1, y_test_pred, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbc9c937-8d7d-47b4-8321-4bea7746c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Model  QWK Score (Train)  QWK Score (Test)\n",
      "3       CatBoost Classifier           0.738946          0.697195\n",
      "4        XGBoost Classifier           0.719882          0.690799\n",
      "1  Random Forest Classifier           0.674805          0.670565\n",
      "0       Logistic Regression           0.671035          0.670444\n",
      "2       AdaBoost Classifier           0.339885          0.335948\n"
     ]
    }
   ],
   "source": [
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='QWK Score (Test)', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a52927b-5982-4847-972a-412c7fbf0aa6",
   "metadata": {},
   "source": [
    "### TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37914218-cc40-4ecb-969e-18f4dedb62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "2    4017\n",
      "1    3022\n",
      "3    2513\n",
      "0     801\n",
      "4     621\n",
      "5     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "2    1005\n",
      "1     756\n",
      "3     628\n",
      "0     200\n",
      "4     155\n",
      "5      25\n",
      "Name: count, dtype: int64\n",
      "Working on Logistic Regression...\n",
      "Elapsed time for Logistic Regression: 7.91997218132019 seconds\n",
      "Distinct predicted values on training set for Logistic Regression: [1 2 3 4 5]\n",
      "Distinct predicted values on test set for Logistic Regression: [1 2 3 4 5]\n",
      "Working on CatBoost Classifier...\n",
      "Elapsed time for CatBoost Classifier: 165.75451135635376 seconds\n",
      "Distinct predicted values on training set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for CatBoost Classifier: [1 2 3 4 5]\n",
      "Working on XGBoost Classifier...\n",
      "Elapsed time for XGBoost Classifier: 354.85085010528564 seconds\n",
      "Distinct predicted values on training set for XGBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for XGBoost Classifier: [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "\n",
    "# Load the datasets\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "df_tfidf = pd.read_csv('tfidf_features.csv')\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = df_tfidf\n",
    "y = df_transformed['score'] - 1  # Adjust class labels to start from 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    #'Random Forest Classifier': RandomForestClassifier(n_estimators=600, max_depth=4, random_state=42),\n",
    "    #'AdaBoost Classifier': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost Classifier': CatBoostClassifier(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Classifier': XGBClassifier(objective='multi:softmax', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "   # 'LightGBM Classifier': LGBMClassifier(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on training set for {name}: {np.unique(y_train_pred)}\")\n",
    "    kappa_train_score = cohen_kappa_score(y_train + 1, y_train_pred, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on test set for {name}: {np.unique(y_test_pred)}\")\n",
    "    kappa_test_score = cohen_kappa_score(y_test + 1, y_test_pred, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be600ae8-4c25-442c-a529-33f12cdb3c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  QWK Score (Train)  QWK Score (Test)\n",
      "0  Logistic Regression           0.663757          0.578316\n",
      "1  CatBoost Classifier           0.788927          0.510062\n",
      "2   XGBoost Classifier           0.993334          0.506426\n"
     ]
    }
   ],
   "source": [
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='QWK Score (Test)', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a2a6a-207d-455a-9b6d-0388032dd7ad",
   "metadata": {},
   "source": [
    "### Combined Numerical and TF IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccc6649c-0190-44c1-b3e2-0861967eb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "2    4017\n",
      "1    3022\n",
      "3    2513\n",
      "0     801\n",
      "4     621\n",
      "5     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "2    1005\n",
      "1     756\n",
      "3     628\n",
      "0     200\n",
      "4     155\n",
      "5      25\n",
      "Name: count, dtype: int64\n",
      "Working on CatBoost Classifier...\n",
      "Elapsed time for CatBoost Classifier: 54.24480748176575 seconds\n",
      "Distinct predicted values on training set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Working on XGBoost Classifier...\n",
      "Elapsed time for XGBoost Classifier: 165.84056162834167 seconds\n",
      "Distinct predicted values on training set for XGBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for XGBoost Classifier: [1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "combined_features_df = pd.read_csv('combined_features.csv')\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = combined_features_df\n",
    "y = df_transformed['score'] - 1  # Adjust class labels to start from 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    #'Random Forest Classifier': RandomForestClassifier(n_estimators=600, max_depth=4, random_state=42),\n",
    "    #'AdaBoost Classifier': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost Classifier': CatBoostClassifier(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Classifier': XGBClassifier(objective='multi:softmax', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Classifier': LGBMClassifier(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on training set for {name}: {np.unique(y_train_pred)}\")\n",
    "    kappa_train_score = cohen_kappa_score(y_train + 1, y_train_pred, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on test set for {name}: {np.unique(y_test_pred)}\")\n",
    "    kappa_test_score = cohen_kappa_score(y_test + 1, y_test_pred, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a44d4a63-ac69-40cb-af88-311c2a27fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  QWK Score (Train)  QWK Score (Test)\n",
      "0  CatBoost Classifier           0.847677          0.728796\n",
      "1   XGBoost Classifier           0.995395          0.723560\n"
     ]
    }
   ],
   "source": [
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='QWK Score (Test)', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a677ecd8-1a38-4aa3-9e15-74959902ecdc",
   "metadata": {},
   "source": [
    "## Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5fb7c5-78b8-4634-b89b-a19243c1f283",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1612cb9d-ce9a-49c1-af5f-414ae1b7eeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n",
      "Working on Linear Regression...\n",
      "Elapsed time for Linear Regression: 0.00824737548828125 seconds\n",
      "Working on Random Forest Regressor...\n",
      "Elapsed time for Random Forest Regressor: 9.106829643249512 seconds\n",
      "Working on AdaBoost Regressor...\n",
      "Elapsed time for AdaBoost Regressor: 0.24966645240783691 seconds\n",
      "Working on CatBoost Regressor...\n",
      "Elapsed time for CatBoost Regressor: 1.296612024307251 seconds\n",
      "Working on XGBoost Regressor...\n",
      "Elapsed time for XGBoost Regressor: 0.6050937175750732 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the training and test datasets\n",
    "df = pd.read_csv('numeric_features_added_v1.csv')\n",
    "\n",
    "# Define numerical features\n",
    "numerical_features = [\n",
    "    'reading_time',\n",
    "    'mistakes_dist_ratio',\n",
    "    'polysyllabcount',\n",
    "    'sentence_count',\n",
    "    'difficult_words',\n",
    "    'comma_count',\n",
    "    'transitional_phrases_c',\n",
    "    'text_dist_words_ratio'\n",
    "]\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = df[numerical_features]\n",
    "y = df['score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=600, max_depth=4, random_state=42),\n",
    "    'AdaBoost Regressor': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost Regressor': CatBoostRegressor(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Regressor': XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Regressor': LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_scaled = (y_train_pred - y_train_pred.min()) / (y_train_pred.max() - y_train_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_train_discretized = discretize_predictions(y_train, target_classes)\n",
    "    y_train_pred_discretized = discretize_predictions(y_train_pred_scaled, target_classes)\n",
    "    kappa_train_score = cohen_kappa_score(y_train_discretized, y_train_pred_discretized, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_scaled = (y_test_pred - y_test_pred.min()) / (y_test_pred.max() - y_test_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_test_discretized = discretize_predictions(y_test, target_classes)\n",
    "    y_test_pred_discretized = discretize_predictions(y_test_pred_scaled, target_classes)\n",
    "    kappa_test_score = cohen_kappa_score(y_test_discretized, y_test_pred_discretized, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a1ce027-5284-4bcf-8cf3-d4412b5ff5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  QWK Score (Train)  QWK Score (Test)\n",
      "0        Linear Regression           0.438766          0.661431\n",
      "1  Random Forest Regressor           0.719541          0.716102\n",
      "2       AdaBoost Regressor           0.716374          0.707997\n",
      "3       CatBoost Regressor           0.738748          0.714213\n",
      "4        XGBoost Regressor           0.760924          0.725878\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91058eed-8c96-4a81-b054-57d848a2133e",
   "metadata": {},
   "source": [
    "### TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d2af201-da3b-4f56-ad1c-add5d301dd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n",
      "Working on Linear Regression...\n",
      "Elapsed time for Linear Regression: 1.5149664878845215 seconds\n",
      "Working on CatBoost Regressor...\n",
      "Elapsed time for CatBoost Regressor: 14.15885615348816 seconds\n",
      "Working on XGBoost Regressor...\n",
      "Elapsed time for XGBoost Regressor: 30.564793825149536 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "df_tfidf = pd.read_csv('tfidf_features.csv')\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = df_tfidf\n",
    "y = df_transformed['score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    # 'Random Forest Regressor': RandomForestRegressor(n_estimators=600, max_depth=4, random_state=42),\n",
    "    # 'AdaBoost Regressor': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost Regressor': CatBoostRegressor(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Regressor': XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Regressor': LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_scaled = (y_train_pred - y_train_pred.min()) / (y_train_pred.max() - y_train_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_train_discretized = discretize_predictions(y_train, target_classes)\n",
    "    y_train_pred_discretized = discretize_predictions(y_train_pred_scaled, target_classes)\n",
    "    kappa_train_score = cohen_kappa_score(y_train_discretized, y_train_pred_discretized, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_scaled = (y_test_pred - y_test_pred.min()) / (y_test_pred.max() - y_test_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_test_discretized = discretize_predictions(y_test, target_classes)\n",
    "    y_test_pred_discretized = discretize_predictions(y_test_pred_scaled, target_classes)\n",
    "    kappa_test_score = cohen_kappa_score(y_test_discretized, y_test_pred_discretized, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eab6d8a7-76cf-4e90-b18a-279eaff8a89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  QWK Score (Train)  QWK Score (Test)\n",
      "0   Linear Regression           0.650407          0.485852\n",
      "1  CatBoost Regressor           0.725470          0.583873\n",
      "2   XGBoost Regressor           0.848442          0.520738\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b278675-2ab9-43b4-b1f7-4765c275817d",
   "metadata": {},
   "source": [
    "### Combined Numerical and TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7fb60408-0d9a-4732-b81d-6f87e4700744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n",
      "Working on Linear Regression...\n",
      "Elapsed time for Linear Regression: 1.3538024425506592 seconds\n",
      "Working on CatBoost Regressor...\n",
      "Elapsed time for CatBoost Regressor: 14.627174615859985 seconds\n",
      "Working on XGBoost Regressor...\n",
      "Elapsed time for XGBoost Regressor: 30.71868872642517 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "combined_features_df = pd.read_csv('combined_features.csv')\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = combined_features_df\n",
    "y = df_transformed['score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    #'Random Forest Regressor': RandomForestRegressor(n_estimators=600, max_depth=4, random_state=42),\n",
    "    #'AdaBoost Regressor': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost Regressor': CatBoostRegressor(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Regressor': XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "   # 'LightGBM Regressor': LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_scaled = (y_train_pred - y_train_pred.min()) / (y_train_pred.max() - y_train_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_train_discretized = discretize_predictions(y_train, target_classes)\n",
    "    y_train_pred_discretized = discretize_predictions(y_train_pred_scaled, target_classes)\n",
    "    kappa_train_score = cohen_kappa_score(y_train_discretized, y_train_pred_discretized, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_scaled = (y_test_pred - y_test_pred.min()) / (y_test_pred.max() - y_test_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_test_discretized = discretize_predictions(y_test, target_classes)\n",
    "    y_test_pred_discretized = discretize_predictions(y_test_pred_scaled, target_classes)\n",
    "    kappa_test_score = cohen_kappa_score(y_test_discretized, y_test_pred_discretized, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b492c254-51b0-4e46-a200-c74288381564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  QWK Score (Train)  QWK Score (Test)\n",
      "0   Linear Regression           0.697232          0.672009\n",
      "1  CatBoost Regressor           0.870019          0.737538\n",
      "2   XGBoost Regressor           0.942193          0.752054\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa272913-2d3e-476e-b4d3-3230fe73e860",
   "metadata": {},
   "source": [
    "## Word2Vect features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1d85d-6041-46e1-91d7-40bf10063296",
   "metadata": {},
   "source": [
    "#### Option 1: Traditional Machine Learning Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7fc0a334-13a3-4045-8d49-238585ee0f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "2    4017\n",
      "1    3022\n",
      "3    2513\n",
      "0     801\n",
      "4     621\n",
      "5     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "2    1005\n",
      "1     756\n",
      "3     628\n",
      "0     200\n",
      "4     155\n",
      "5      25\n",
      "Name: count, dtype: int64\n",
      "Working on Random Forest Classifier...\n",
      "Elapsed time for Random Forest Classifier: 55.92894721031189 seconds\n",
      "Distinct predicted values on training set for Random Forest Classifier: [1 2 3 4]\n",
      "Distinct predicted values on test set for Random Forest Classifier: [1 2 3 4]\n",
      "Working on AdaBoost Classifier...\n",
      "Elapsed time for AdaBoost Classifier: 106.02817106246948 seconds\n",
      "Distinct predicted values on training set for AdaBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for AdaBoost Classifier: [1 2 3 4 5 6]\n",
      "Working on CatBoost Classifier...\n",
      "Elapsed time for CatBoost Classifier: 14.567898750305176 seconds\n",
      "Distinct predicted values on training set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Working on XGBoost Classifier...\n",
      "Elapsed time for XGBoost Classifier: 31.673995971679688 seconds\n",
      "Distinct predicted values on training set for XGBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for XGBoost Classifier: [1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# Load the datasets\n",
    "embeddings_df = pd.read_csv('word2vec_features.csv')\n",
    "\n",
    "# Function to convert space-separated string of numbers (with brackets) to a list of floats\n",
    "def convert_to_list(embedding_str):\n",
    "    # Remove the square brackets\n",
    "    embedding_str = embedding_str.replace('[', '').replace(']', '')\n",
    "    # Split the string by spaces and convert to list of floats\n",
    "    return [float(num) for num in embedding_str.split()]\n",
    "\n",
    "# Apply the function to the word2vec_embedding column\n",
    "embeddings_df['word2vec_embedding'] = embeddings_df['word2vec_embedding'].apply(convert_to_list)\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = np.vstack(embeddings_df['word2vec_embedding'].values)\n",
    "y = embeddings_df['score'] - 1  # Adjust class labels to start from 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest Classifier': RandomForestClassifier(n_estimators=600, max_depth=4, random_state=42),\n",
    "    'AdaBoost Classifier': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost Classifier': CatBoostClassifier(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Classifier': XGBClassifier(objective='multi:softmax', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Classifier': LGBMClassifier(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on training set for {name}: {np.unique(y_train_pred)}\")\n",
    "    kappa_train_score = cohen_kappa_score(y_train + 1, y_train_pred, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on test set for {name}: {np.unique(y_test_pred)}\")\n",
    "    kappa_test_score = cohen_kappa_score(y_test + 1, y_test_pred, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "be4254cf-cbb9-4b80-ba3b-3e3077576e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Model  QWK Score (Train)  QWK Score (Test)\n",
      "3        XGBoost Classifier           0.887020          0.567956\n",
      "2       CatBoost Classifier           0.701877          0.557159\n",
      "1       AdaBoost Classifier           0.426707          0.391434\n",
      "0  Random Forest Classifier           0.267254          0.250111\n"
     ]
    }
   ],
   "source": [
    "# Display results in a DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(by='QWK Score (Test)', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdd6f0-6e0e-4d18-8f1e-31344d022908",
   "metadata": {},
   "source": [
    "#### Option 2: Traditional Machine Learning Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d656861c-7d4c-4a2d-ba0e-c5c5b7ae8f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n",
      "Working on Linear Regression...\n",
      "Elapsed time for Linear Regression: 0.33718204498291016 seconds\n",
      "Working on CatBoost Regressor...\n",
      "Elapsed time for CatBoost Regressor: 5.67457389831543 seconds\n",
      "Working on XGBoost Regressor...\n",
      "Elapsed time for XGBoost Regressor: 4.95456862449646 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import time\n",
    "import ast\n",
    "\n",
    "# Load the datasets\n",
    "embeddings_df = pd.read_csv('word2vec_features.csv')\n",
    "\n",
    "# Function to convert space-separated string of numbers (with brackets) to a list of floats\n",
    "def convert_to_list(embedding_str):\n",
    "    # Remove the square brackets\n",
    "    embedding_str = embedding_str.replace('[', '').replace(']', '')\n",
    "    # Split the string by spaces and convert to list of floats\n",
    "    return [float(num) for num in embedding_str.split()]\n",
    "\n",
    "# Apply the function to the word2vec_embedding column\n",
    "embeddings_df['word2vec_embedding'] = embeddings_df['word2vec_embedding'].apply(convert_to_list)\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = np.vstack(embeddings_df['word2vec_embedding'].values)\n",
    "y = embeddings_df['score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    #'Random Forest Regressor': RandomForestRegressor(n_estimators=600, max_depth=4, random_state=42),\n",
    "    #'AdaBoost Regressor': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost Regressor': CatBoostRegressor(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Regressor': XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Regressor': LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_scaled = (y_train_pred - y_train_pred.min()) / (y_train_pred.max() - y_train_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_train_discretized = discretize_predictions(y_train, target_classes)\n",
    "    y_train_pred_discretized = discretize_predictions(y_train_pred_scaled, target_classes)\n",
    "    kappa_train_score = cohen_kappa_score(y_train_discretized, y_train_pred_discretized, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_scaled = (y_test_pred - y_test_pred.min()) / (y_test_pred.max() - y_test_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_test_discretized = discretize_predictions(y_test, target_classes)\n",
    "    y_test_pred_discretized = discretize_predictions(y_test_pred_scaled, target_classes)\n",
    "    kappa_test_score = cohen_kappa_score(y_test_discretized, y_test_pred_discretized, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4a7f0c32-9acf-42a4-a6e9-c45c8f468a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  QWK Score (Train)  QWK Score (Test)\n",
      "0   Linear Regression           0.355899          0.351317\n",
      "1  CatBoost Regressor           0.523580          0.373310\n",
      "2   XGBoost Regressor           0.673253          0.461169\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe861dc-2505-416a-be14-538d9311f1ba",
   "metadata": {},
   "source": [
    "#### Option 3: Using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e8cf6d-c6b6-47c2-a69c-a964103d85be",
   "metadata": {},
   "source": [
    "A simple feedforward neural network using TensorFlow Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "538e6726-bf85-48dc-a47a-6cd6d184e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "347/347 - 7s - 21ms/step - loss: 1.5953 - val_loss: 1.0994\n",
      "Epoch 2/50\n",
      "347/347 - 2s - 5ms/step - loss: 1.1590 - val_loss: 1.1863\n",
      "Epoch 3/50\n",
      "347/347 - 2s - 5ms/step - loss: 1.0383 - val_loss: 1.0890\n",
      "Epoch 4/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.9696 - val_loss: 1.1958\n",
      "Epoch 5/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.9548 - val_loss: 1.0846\n",
      "Epoch 6/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.8994 - val_loss: 1.1531\n",
      "Epoch 7/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.8765 - val_loss: 0.9718\n",
      "Epoch 8/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.8655 - val_loss: 1.0934\n",
      "Epoch 9/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.8411 - val_loss: 0.8995\n",
      "Epoch 10/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.8187 - val_loss: 0.9938\n",
      "Epoch 11/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.8122 - val_loss: 0.9161\n",
      "Epoch 12/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.8045 - val_loss: 0.8496\n",
      "Epoch 13/50\n",
      "347/347 - 2s - 7ms/step - loss: 0.7992 - val_loss: 0.9131\n",
      "Epoch 14/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7804 - val_loss: 0.8731\n",
      "Epoch 15/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7748 - val_loss: 0.8712\n",
      "Epoch 16/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7676 - val_loss: 0.8110\n",
      "Epoch 17/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.7677 - val_loss: 0.9029\n",
      "Epoch 18/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7550 - val_loss: 0.8325\n",
      "Epoch 19/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7534 - val_loss: 0.8141\n",
      "Epoch 20/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.7355 - val_loss: 0.7821\n",
      "Epoch 21/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.7377 - val_loss: 0.8552\n",
      "Epoch 22/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.7360 - val_loss: 0.7710\n",
      "Epoch 23/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7272 - val_loss: 0.8388\n",
      "Epoch 24/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7228 - val_loss: 0.7627\n",
      "Epoch 25/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.7127 - val_loss: 0.7546\n",
      "Epoch 26/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.7113 - val_loss: 0.7652\n",
      "Epoch 27/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.7109 - val_loss: 0.7226\n",
      "Epoch 28/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.7012 - val_loss: 0.7522\n",
      "Epoch 29/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.7016 - val_loss: 0.7102\n",
      "Epoch 30/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.6955 - val_loss: 0.7275\n",
      "Epoch 31/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.6823 - val_loss: 0.7573\n",
      "Epoch 32/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.6947 - val_loss: 0.7301\n",
      "Epoch 33/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6810 - val_loss: 0.7435\n",
      "Epoch 34/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.6759 - val_loss: 0.7582\n",
      "Epoch 35/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6706 - val_loss: 0.7326\n",
      "Epoch 36/50\n",
      "347/347 - 2s - 5ms/step - loss: 0.6742 - val_loss: 0.8197\n",
      "Epoch 37/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6716 - val_loss: 0.7665\n",
      "Epoch 38/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6759 - val_loss: 0.6820\n",
      "Epoch 39/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6575 - val_loss: 0.7014\n",
      "Epoch 40/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.6600 - val_loss: 0.6928\n",
      "Epoch 41/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.6517 - val_loss: 0.7031\n",
      "Epoch 42/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.6594 - val_loss: 0.6703\n",
      "Epoch 43/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6512 - val_loss: 0.7134\n",
      "Epoch 44/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.6468 - val_loss: 0.7217\n",
      "Epoch 45/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6426 - val_loss: 0.7273\n",
      "Epoch 46/50\n",
      "347/347 - 2s - 6ms/step - loss: 0.6378 - val_loss: 0.6923\n",
      "Epoch 47/50\n",
      "347/347 - 3s - 7ms/step - loss: 0.6332 - val_loss: 0.6852\n",
      "Epoch 48/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.6416 - val_loss: 0.6702\n",
      "Epoch 49/50\n",
      "347/347 - 2s - 7ms/step - loss: 0.6332 - val_loss: 0.6927\n",
      "Epoch 50/50\n",
      "347/347 - 3s - 8ms/step - loss: 0.6287 - val_loss: 0.6538\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "QWK Score on Train Set: 0.6412155326583289\n",
      "QWK Score on Test Set: 0.5535504981689063\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the datasets\n",
    "embeddings_df = pd.read_csv('word2vec_features.csv')\n",
    "\n",
    "# Function to convert space-separated string of numbers (with brackets) to a list of floats\n",
    "def convert_to_list(embedding_str):\n",
    "    # Remove the square brackets\n",
    "    embedding_str = embedding_str.replace('[', '').replace(']', '')\n",
    "    # Split the string by spaces and convert to list of floats\n",
    "    return [float(num) for num in embedding_str.split()]\n",
    "\n",
    "# Apply the function to the word2vec_embedding column\n",
    "embeddings_df['word2vec_embedding'] = embeddings_df['word2vec_embedding'].apply(convert_to_list)\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X = np.vstack(embeddings_df['word2vec_embedding'].values)\n",
    "y = embeddings_df['score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dynamically determine target classes\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Standardize data for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = model.predict(X_train_scaled).flatten()\n",
    "y_test_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "# Define the QWK computation function\n",
    "def compute_qwk(y_true, y_pred, target_classes):\n",
    "    y_pred_discretized = discretize_predictions(y_pred, target_classes)\n",
    "    return cohen_kappa_score(y_true, y_pred_discretized, weights='quadratic')\n",
    "\n",
    "# Evaluate using QWK score\n",
    "qwk_train = compute_qwk(y_train, y_train_pred, target_classes)\n",
    "qwk_test = compute_qwk(y_test, y_test_pred, target_classes)\n",
    "\n",
    "print(f\"QWK Score on Train Set: {qwk_train}\")\n",
    "print(f\"QWK Score on Test Set: {qwk_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949be7a-0fdd-412f-b9e4-351a2707aa60",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410564ba-4326-49d9-aaac-31874995911f",
   "metadata": {},
   "source": [
    "- **Run Time and Resource Usage:** The BERT model occupied all resources and required several hours to run, leading to the decision to abandon this approach due to impracticality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0aed64-92bb-49b6-a75d-7b435633e25e",
   "metadata": {},
   "source": [
    "#### Option 1: Traditional Machine Learning Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bf6410cc-cf22-4745-aea0-f74ce1e123ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "2    4017\n",
      "1    3022\n",
      "3    2513\n",
      "0     801\n",
      "4     621\n",
      "5     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "2    1005\n",
      "1     756\n",
      "3     628\n",
      "0     200\n",
      "4     155\n",
      "5      25\n",
      "Name: count, dtype: int64\n",
      "Working on CatBoost Classifier...\n",
      "Elapsed time for CatBoost Classifier: 38.15868878364563 seconds\n",
      "Distinct predicted values on training set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for CatBoost Classifier: [1 2 3 4 5 6]\n",
      "Working on XGBoost Classifier...\n",
      "Elapsed time for XGBoost Classifier: 81.05140852928162 seconds\n",
      "Distinct predicted values on training set for XGBoost Classifier: [1 2 3 4 5 6]\n",
      "Distinct predicted values on test set for XGBoost Classifier: [1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "\n",
    "# Load the datasets\n",
    "embeddings_df = pd.read_csv('bert_features.csv')\n",
    "\n",
    "# Load the original dataset with scores to ensure the labels match the embeddings\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "# Assuming embeddings_df already contains embeddings as numerical columns\n",
    "# Combine BERT embeddings with the scores\n",
    "y = df_transformed['score'] - 1  # Adjust class labels to start from 0\n",
    "X = embeddings_df.values\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    #'Random Forest Classifier': RandomForestClassifier(n_estimators=600, max_depth=4, random_state=42),\n",
    "    #'AdaBoost Classifier': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost Classifier': CatBoostClassifier(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Classifier': XGBClassifier(objective='multi:softmax', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Classifier': LGBMClassifier(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on training set for {name}: {np.unique(y_train_pred)}\")\n",
    "    kappa_train_score = cohen_kappa_score(y_train + 1, y_train_pred, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test) + 1  # Adjust predictions back to original scale\n",
    "    print(f\"Distinct predicted values on test set for {name}: {np.unique(y_test_pred)}\")\n",
    "    kappa_test_score = cohen_kappa_score(y_test + 1, y_test_pred, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "20e31436-6253-4c64-b1b8-9e101840ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  QWK Score (Train)  QWK Score (Test)\n",
      "0  CatBoost Classifier           0.823684          0.679635\n",
      "1   XGBoost Classifier           0.975234          0.690236\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da8944-12f0-41b2-a8e7-67e74744f9f2",
   "metadata": {},
   "source": [
    "#### Option 2: Traditional Machine Learning Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d80b5149-1d74-4fc8-913c-4621c3ffdefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n",
      "Working on Linear Regression...\n",
      "Elapsed time for Linear Regression: 21291.91133761406 seconds\n",
      "Working on CatBoost Regressor...\n",
      "Elapsed time for CatBoost Regressor: 25.162798404693604 seconds\n",
      "Working on XGBoost Regressor...\n",
      "Elapsed time for XGBoost Regressor: 28.569011688232422 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import time\n",
    "\n",
    "# Load the BERT embeddings\n",
    "embeddings_df = pd.read_csv('bert_features.csv')\n",
    "\n",
    "# Load the original dataset with scores to ensure the labels match the embeddings\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "# Combine BERT embeddings with the scores\n",
    "y = df_transformed['score']\n",
    "X = embeddings_df.values\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'CatBoost Regressor': CatBoostRegressor(iterations=600, depth=4, learning_rate=0.1, random_seed=42, silent=True),\n",
    "    'XGBoost Regressor': XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1, max_depth=4, alpha=10, n_estimators=600),\n",
    "    #'LightGBM Regressor': LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Working on {name}...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed time for {name}: {end_time - start_time} seconds\")\n",
    "\n",
    "    # Predictions on training set\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_pred_scaled = (y_train_pred - y_train_pred.min()) / (y_train_pred.max() - y_train_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_train_discretized = discretize_predictions(y_train, target_classes)\n",
    "    y_train_pred_discretized = discretize_predictions(y_train_pred_scaled, target_classes)\n",
    "    kappa_train_score = cohen_kappa_score(y_train_discretized, y_train_pred_discretized, weights='quadratic')\n",
    "\n",
    "    # Predictions on test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_pred_scaled = (y_test_pred - y_test_pred.min()) / (y_test_pred.max() - y_test_pred.min()) * (y_train.max() - y_train.min()) + y_train.min()\n",
    "    y_test_discretized = discretize_predictions(y_test, target_classes)\n",
    "    y_test_pred_discretized = discretize_predictions(y_test_pred_scaled, target_classes)\n",
    "    kappa_test_score = cohen_kappa_score(y_test_discretized, y_test_pred_discretized, weights='quadratic')\n",
    "\n",
    "    results.append({'Model': name, 'QWK Score (Train)': kappa_train_score, 'QWK Score (Test)': kappa_test_score})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01e2f164-da8f-43c3-b9ec-a28b09b46820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  QWK Score (Train)  QWK Score (Test)\n",
      "0   Linear Regression           0.557390          0.506605\n",
      "1  CatBoost Regressor           0.756849          0.591912\n",
      "2   XGBoost Regressor           0.796409          0.596577\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a748b0b-6f87-4e93-ae48-75107c38de4c",
   "metadata": {},
   "source": [
    "#### Option 3: Using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abdc9ccd-2d03-4565-a2f9-36a8e168bab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split, Train, Validate\n",
      "Distribution of target classes in the training data:\n",
      "score\n",
      "3    4017\n",
      "2    3022\n",
      "4    2513\n",
      "1     801\n",
      "5     621\n",
      "6     100\n",
      "Name: count, dtype: int64\n",
      "Distribution of target classes in the test data:\n",
      "score\n",
      "3    1005\n",
      "2     756\n",
      "4     628\n",
      "1     200\n",
      "5     155\n",
      "6      25\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dziya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347/347 - 2s - 6ms/step - loss: 1.2502 - val_loss: 0.9882\n",
      "Epoch 2/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.7721 - val_loss: 0.9516\n",
      "Epoch 3/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.6475 - val_loss: 0.8703\n",
      "Epoch 4/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.5889 - val_loss: 0.8411\n",
      "Epoch 5/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.5490 - val_loss: 1.0387\n",
      "Epoch 6/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.5269 - val_loss: 0.8222\n",
      "Epoch 7/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.5002 - val_loss: 0.7037\n",
      "Epoch 8/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4840 - val_loss: 0.8296\n",
      "Epoch 9/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4688 - val_loss: 0.8837\n",
      "Epoch 10/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4594 - val_loss: 0.8370\n",
      "Epoch 11/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.4582 - val_loss: 0.5535\n",
      "Epoch 12/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4458 - val_loss: 0.9599\n",
      "Epoch 13/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4476 - val_loss: 0.7568\n",
      "Epoch 14/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4317 - val_loss: 0.6697\n",
      "Epoch 15/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4297 - val_loss: 0.6492\n",
      "Epoch 16/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4278 - val_loss: 0.6833\n",
      "Epoch 17/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4114 - val_loss: 0.7847\n",
      "Epoch 18/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4128 - val_loss: 0.6763\n",
      "Epoch 19/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4029 - val_loss: 0.6514\n",
      "Epoch 20/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.4018 - val_loss: 0.8097\n",
      "Epoch 21/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3929 - val_loss: 0.7097\n",
      "Epoch 22/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3950 - val_loss: 0.6370\n",
      "Epoch 23/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3807 - val_loss: 0.7214\n",
      "Epoch 24/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3817 - val_loss: 0.6318\n",
      "Epoch 25/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3704 - val_loss: 0.6557\n",
      "Epoch 26/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3652 - val_loss: 0.6207\n",
      "Epoch 27/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.3709 - val_loss: 0.7520\n",
      "Epoch 28/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3667 - val_loss: 0.6769\n",
      "Epoch 29/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3494 - val_loss: 0.6075\n",
      "Epoch 30/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3504 - val_loss: 0.6055\n",
      "Epoch 31/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.3454 - val_loss: 0.6439\n",
      "Epoch 32/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3400 - val_loss: 0.7151\n",
      "Epoch 33/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3433 - val_loss: 0.6272\n",
      "Epoch 34/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3352 - val_loss: 0.7017\n",
      "Epoch 35/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.3327 - val_loss: 0.8116\n",
      "Epoch 36/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3285 - val_loss: 0.7277\n",
      "Epoch 37/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3209 - val_loss: 0.6538\n",
      "Epoch 38/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3201 - val_loss: 0.6794\n",
      "Epoch 39/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.3252 - val_loss: 0.6478\n",
      "Epoch 40/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3113 - val_loss: 0.7430\n",
      "Epoch 41/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3056 - val_loss: 0.6934\n",
      "Epoch 42/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.3024 - val_loss: 0.6951\n",
      "Epoch 43/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.3024 - val_loss: 0.7192\n",
      "Epoch 44/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.2974 - val_loss: 0.6520\n",
      "Epoch 45/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.2975 - val_loss: 0.6626\n",
      "Epoch 46/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.2922 - val_loss: 0.6979\n",
      "Epoch 47/50\n",
      "347/347 - 1s - 4ms/step - loss: 0.2871 - val_loss: 0.6694\n",
      "Epoch 48/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.2855 - val_loss: 0.7776\n",
      "Epoch 49/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.2772 - val_loss: 0.7843\n",
      "Epoch 50/50\n",
      "347/347 - 1s - 2ms/step - loss: 0.2857 - val_loss: 0.6415\n",
      "\u001b[1m347/347\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step\n",
      "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Load the BERT embeddings\n",
    "embeddings_df = pd.read_csv('bert_features.csv')\n",
    "\n",
    "# Load the original dataset with scores to ensure the labels match the embeddings\n",
    "df_transformed = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "# Combine BERT embeddings with the scores\n",
    "y = df_transformed['score']\n",
    "X = embeddings_df.values\n",
    "\n",
    "print(\"Working on Split, Train, Validate\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dynamically determine target classes\n",
    "target_classes = np.sort(np.unique(y))\n",
    "\n",
    "# Check the distribution of the target classes in the training data\n",
    "print(\"Distribution of target classes in the training data:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Check the distribution of the target classes in the test data\n",
    "print(\"Distribution of target classes in the test data:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Standardize data for neural networks\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=2)\n",
    "\n",
    "# Predict\n",
    "y_train_pred = model.predict(X_train_scaled).flatten()\n",
    "y_test_pred = model.predict(X_test_scaled).flatten()\n",
    "\n",
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "# Define the QWK computation function\n",
    "def compute_qwk(y_true, y_pred, target_classes):\n",
    "    y_pred_discretized = discretize_predictions(y_pred, target_classes)\n",
    "    return cohen_kappa_score(y_true, y_pred_discretized, weights='quadratic')\n",
    "\n",
    "# Evaluate using QWK score\n",
    "qwk_train = compute_qwk(y_train, y_train_pred, target_classes)\n",
    "qwk_test = compute_qwk(y_test, y_test_pred, target_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a082253e-3747-4ca1-abdb-61fc4c8cb8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK Score on Train Set: 0.7216489652760272\n",
      "QWK Score on Test Set: 0.5787502629917947\n"
     ]
    }
   ],
   "source": [
    "print(f\"QWK Score on Train Set: {qwk_train}\")\n",
    "print(f\"QWK Score on Test Set: {qwk_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da058da4-f26d-48c6-95a3-b442e48090b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
