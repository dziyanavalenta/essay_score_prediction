{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8afaea-3f14-4093-bd89-66dbe256fd9e",
   "metadata": {},
   "source": [
    "## README\n",
    "This Jupyter Notebook performs the following tasks:\n",
    "\n",
    "1. **Import Libraries**: Imports necessary libraries for data manipulation, text processing, and multiprocessing.\n",
    "   \n",
    "2. **Get Data**: Reads the input data from a CSV file.\n",
    "\n",
    "3. **Preprocess Data**:\n",
    "    - **Extract Distinct Words with Apostrophe**: Finds and lists all distinct words containing apostrophes.\n",
    "    - **Find Transitional Phrases**: Identifies and counts transitional phrases in the text.\n",
    "    - **General Text Counting Functions**: Includes functions for counting words, phrases, stopwords, characters, and various other text features.\n",
    "\n",
    "4. **Feature Engineering**:\n",
    "    - **Text Analysis Features**: Computes various text features such as word count, stopword count, punctuation count, sentence lengths, and others.\n",
    "    - **Ratio Features**: Calculates ratios of different features like distinct words ratio, mistakes ratio, and transitional phrases ratio.\n",
    "    - **Text Statistics Features**: Uses the `textstat` library to compute readability and complexity metrics such as Flesch reading ease, SMOG index, Coleman-Liau index, and others.\n",
    "\n",
    "5. **Export Data**: Exports the processed DataFrame with the new features to a CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdaa90e-fe69-4f93-80ef-3c9d48e61de3",
   "metadata": {},
   "source": [
    "## Content\n",
    "  - [1. Import Libraries](#1-Import-Libraries)\n",
    "  - [2. Get Data](#2-Get-Data)\n",
    "  - [3. Preprocess Data](#3-Preprocess-Data)\n",
    "    - [Extract Distinct Words with Apostrophe](#Extract-Distinct-Words-with-Apostrophe)\n",
    "    - [Find Transitional Phrases](#Find-Transitional-Phrases)\n",
    "    - [General Text Counting Functions](#General-Text-Counting-Functions)\n",
    "  - [4. Feature Engineering](#4-Feature-Engineering)\n",
    "    - [Text Analysis Features](#Text-Analysis-Features)\n",
    "    - [Ratio Features](#Ratio-Features)\n",
    "    - [Text Statistics Features](#Text-Statistics-Features)\n",
    "  - [5. Export Data](#5-Export-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b4e9f3-7ff5-41b2-942a-f7742c4a90cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 32\n",
      "Working on Text Analysis Features\n",
      "Elapsed time: 55.57690668106079 seconds\n",
      "Working on Ratio Features\n",
      "Elapsed time: 2.8294968605041504 seconds\n",
      "Working on Text Statistics Features\n",
      "Elapsed time: 22.818469047546387 seconds\n",
      "Exporting file\n",
      "File exported\n"
     ]
    }
   ],
   "source": [
    "# 1. Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import time\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import textstat\n",
    "\n",
    "# Get the number of available CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of available CPU cores: {num_cores}\")\n",
    "\n",
    "# 2. Get data\n",
    "df = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Transitional phrases list\n",
    "transitional_phrases = [\n",
    "    'Above all', 'Accordingly', 'Additionally', 'After', 'After all', 'Afterward', 'All in all', 'Also', 'Alternatively', \n",
    "    'As a result', 'As an illustration', 'As long as', 'As mentioned earlier', 'As noted', 'At the same time', 'Before', \n",
    "    'Besides', 'But', 'By all means', 'Consequently', 'Conversely', 'Correspondingly', 'Despite', 'During', 'Even if', \n",
    "    'Even so', 'Especially', 'Eventually', 'Finally', 'First', 'For example', 'For instance', 'Furthermore', 'Hence', \n",
    "    'However', 'If', 'In addition', 'In brief', 'In case', 'In comparison', 'In conclusion', 'In fact', 'In contrast', \n",
    "    'In other words', 'In particular', 'In simpler terms', 'In summary', 'In the meantime', 'In the same way', 'Indeed', \n",
    "    'Instead', 'Lastly', 'Later', 'Likewise', 'Meanwhile', 'Moreover', 'More importantly', 'Namely', 'Nevertheless', \n",
    "    'Next', 'Nonetheless', 'Notably', 'Now', 'On the contrary', 'On condition that', 'On one hand', 'On the other hand', \n",
    "    'Overall', 'Particularly', 'Plus', 'Previously', 'Provided that', 'Regardless', 'Second', 'Similarly', 'Since', \n",
    "    'Specifically', 'Still', 'Subsequently', 'That is', 'Then', 'Therefore', 'Third', 'Thus', 'To clarify', 'To conclude', \n",
    "    'To demonstrate', 'To illustrate', 'To put it another way', 'To summarize', 'To sum up', 'Ultimately', 'Unless', 'Unlike', \n",
    "    'Until', 'Whereas', 'Yet', 'Above and beyond', 'According to', 'After a while', 'All things considered', 'Although', \n",
    "    'Another key point', 'As a consequence', 'As a matter of fact', 'As can be seen', 'As far as', 'As soon as', 'At first', \n",
    "    'At last', 'At length', 'At this point', 'Be that as it may', 'By and large', 'By the same token', 'Even though', \n",
    "    'For fear that', 'For that reason', 'For the most part', 'Granted', 'Henceforth', 'If by chance', 'If so', 'In a moment', \n",
    "    'In any case', 'In any event', 'In light of', 'In order to', 'In particular', 'In reality', 'In short', 'In spite of', \n",
    "    'In view of', 'It follows that', 'Least of all', 'Most importantly', 'Needless to say', 'Of course', 'On the whole', \n",
    "    'One example is', 'One reason is', 'Or', 'Over time', 'Prior to', 'Provided that', 'Seeing that', 'So as to', 'Sooner or later', \n",
    "    'Such as', 'That being said', 'The next step', 'Thereafter', 'Thereby', 'Thirdly', 'Through', 'Till', 'To be sure', \n",
    "    'To begin with', 'To illustrate', 'To reiterate', 'To the end that', 'To this end', 'Until now', 'Up to now', 'What is more', \n",
    "    'Without a doubt', 'Without delay', 'Without exception', 'Yet again'\n",
    "]\n",
    "\n",
    "# 3. Preprocess data\n",
    "# Function to extract distinct words with apostrophe\n",
    "def extract_distinct_words_with_apostrophe(full_text):\n",
    "    pattern = r\"\\b\\w*'\\w*\\b\"\n",
    "    words_with_apostrophe = set()\n",
    "    \n",
    "    if isinstance(full_text, str):\n",
    "        texts = [full_text]\n",
    "    else:\n",
    "        texts = full_text\n",
    "    \n",
    "    for text in texts:\n",
    "        matches = re.findall(pattern, text)\n",
    "        words_with_apostrophe.update(matches)\n",
    "    \n",
    "    return sorted(words_with_apostrophe)\n",
    "\n",
    "# Function to find transitional phrases in text\n",
    "def find_transitional_phrases(text):\n",
    "    return [phrase for phrase in transitional_phrases if phrase.lower() in text.lower()]\n",
    "\n",
    "# General text counting functions\n",
    "def count_phrases_in_list(phrases_list):\n",
    "    return len(phrases_list)\n",
    "\n",
    "def count_distinct_phrases(phrases_list):\n",
    "    return len(set(phrases_list))\n",
    "\n",
    "def count_mistakes(word_list):\n",
    "    return len(word_list)\n",
    "\n",
    "def count_distinct_mistakes(word_list):\n",
    "    return len(set(word_list))\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_distinct_words(text):\n",
    "    words = text.lower().split()\n",
    "    return len(set(words))\n",
    "\n",
    "def count_occurrences(text, pattern):\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "def count_character(text, character):\n",
    "    return text.count(character)\n",
    "\n",
    "def count_stopwords(text):\n",
    "    words = text.lower().split()\n",
    "    return len([word for word in words if word in stop_words])\n",
    "\n",
    "def count_newlines(text):\n",
    "    return len([line for line in text.splitlines() if line.strip()])\n",
    "\n",
    "def count_capital_words(text):\n",
    "    words = text.split()\n",
    "    return sum(word[0].isupper() for word in words)\n",
    "\n",
    "def count_letters(text):\n",
    "    return sum(char.isalpha() for char in text)\n",
    "\n",
    "def count_capital_letters(text):\n",
    "    return sum(char.isupper() for char in text)\n",
    "\n",
    "def sentence_lengths(text):\n",
    "    sentences = re.split(r'[.!?]\\s*', text)\n",
    "    lengths = [len(sentence.split()) for sentence in sentences if sentence]\n",
    "    return lengths\n",
    "\n",
    "def min_sentence_length(text):\n",
    "    lengths = sentence_lengths(text)\n",
    "    return min(lengths) if lengths else 0\n",
    "\n",
    "def max_sentence_length(text):\n",
    "    lengths = sentence_lengths(text)\n",
    "    return max(lengths) if lengths else 0\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    lengths = sentence_lengths(text)\n",
    "    return np.mean(lengths) if lengths else 0\n",
    "\n",
    "# 4. Feature Engineering\n",
    "## Text Analysis Features\n",
    "print(\"Working on Text Analysis Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['word_count_in_full_text'] = df['full_text'].str.split().apply(len)\n",
    "df[\"stopword_count_in_full_text\"] = df[\"full_text\"].apply(count_stopwords)\n",
    "df['newline_count'] = df['full_text'].str.count('\\n')\n",
    "df['not_count'] = df['full_text'].str.lower().str.count('not')\n",
    "df['question_count'] = df['full_text'].str.count('\\?')\n",
    "df['comma_count'] = df['full_text'].str.count(',')\n",
    "df['colon_count'] = df['full_text'].str.count(':')\n",
    "df['exclamation_count'] = df['full_text'].str.count('!')\n",
    "df['dash_count'] = df['full_text'].str.count('-')\n",
    "df['capital_word_count'] = df['full_text'].apply(count_capital_words)\n",
    "df['letters_count'] = df['full_text'].apply(count_letters)\n",
    "df['capital_letters_count'] = df['full_text'].apply(count_capital_letters)\n",
    "df['min_sentence_length'] = df['full_text'].apply(min_sentence_length)\n",
    "df['max_sentence_length'] = df['full_text'].apply(max_sentence_length)\n",
    "df['avg_sentence_length'] = df['full_text'].apply(avg_sentence_length)\n",
    "df['words_with_apostrophe'] = df['preprocessed_text_part1'].apply(extract_distinct_words_with_apostrophe)\n",
    "df['transitional_phrases'] = df['preprocessed_text'].apply(find_transitional_phrases)\n",
    "df['mistakes_count'] = df['misspelled_words_spell_checker'].apply(count_mistakes)\n",
    "df['mistakes_dist_count'] = df['misspelled_words_spell_checker'].apply(count_distinct_mistakes)\n",
    "df['transitional_phrases_c'] = df['transitional_phrases'].apply(count_phrases_in_list)\n",
    "df['transitional_phrases_dist_c'] = df['transitional_phrases'].apply(count_distinct_phrases)\n",
    "df['words_with_apostrophe_count'] = df['words_with_apostrophe'].apply(count_phrases_in_list)\n",
    "df['preprocessed_text_count'] = df['preprocessed_text'].apply(count_words)\n",
    "df['preprocessed_text_dist_count'] = df['preprocessed_text'].apply(count_distinct_words)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "## Ratio Features\n",
    "print(\"Working on Ratio Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['text_dist_words_ratio'] = df.apply(lambda x: x['preprocessed_text_dist_count'] / x['preprocessed_text_count'], axis=1)\n",
    "df['mistakes_dist_ratio'] = df.apply(lambda x: x['mistakes_dist_count'] / x['preprocessed_text_count'], axis=1)\n",
    "df['mistakes_dist_dist_ratio'] = df.apply(lambda x: x['mistakes_dist_count'] / x['preprocessed_text_dist_count'], axis=1)\n",
    "df['transitional_phrases_ratio'] = df.apply(lambda x: x['transitional_phrases_c'] / x['preprocessed_text_count'], axis=1)\n",
    "df['transitional_dist_dist_phrases_ratio'] = df.apply(lambda x: x['transitional_phrases_dist_c'] / x['preprocessed_text_count'], axis=1)\n",
    "df['transitional_dist_phrases_ratio'] = df.apply(lambda x: x['transitional_phrases_dist_c'] / x['preprocessed_text_dist_count'], axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "## Text Statistics Features\n",
    "print(\"Working on Text Statistics Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "def textstat_features(text):\n",
    "    features = {}  \n",
    "    features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)  \n",
    "    features['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)  \n",
    "    features['smog_index'] = textstat.smog_index(text)  \n",
    "    features['coleman_liau_index'] = textstat.coleman_liau_index(text)  \n",
    "    features['automated_readability_index'] = textstat.automated_readability_index(text)  \n",
    "    features['dale_chall_readability_score'] = textstat.dale_chall_readability_score(text)  \n",
    "    features['difficult_words'] = textstat.difficult_words(text)  \n",
    "    features['linsear_write_formula'] = textstat.linsear_write_formula(text)  \n",
    "    features['gunning_fog'] = textstat.gunning_fog(text)  \n",
    "    features['text_standard'] = textstat.text_standard(text, float_output=True)  \n",
    "    features['spache_readability'] = textstat.spache_readability(text)  \n",
    "    features['mcalpine_eflaw'] = textstat.mcalpine_eflaw(text)  \n",
    "    features['reading_time'] = textstat.reading_time(text)  \n",
    "    features['syllable_count'] = textstat.syllable_count(text)  \n",
    "    features['lexicon_count'] = textstat.lexicon_count(text)  \n",
    "    features['monosyllabcount'] = textstat.monosyllabcount(text)  \n",
    "    features['char_count'] = textstat.char_count(text)  \n",
    "    features['sentence_count'] = textstat.sentence_count(text)  \n",
    "    features['polysyllabcount'] = textstat.polysyllabcount(text)  \n",
    "    features['reading_time_minutes'] = textstat.reading_time(text) / 60  \n",
    "    return features  \n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(textstat_features)(text) for text in df['full_text_without_non_ascii'])\n",
    "features_df = pd.DataFrame(results)\n",
    "df = pd.concat([df, features_df], axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "# 5. Export Data\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "print(\"Exporting file\")\n",
    "\n",
    "df.to_csv('numeric_features_added_v1.csv', index=False)\n",
    "print(\"File exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145fd49-31ec-460a-befd-29015bdaeb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
