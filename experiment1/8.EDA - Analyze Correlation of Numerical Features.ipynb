{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6c6c4b-a3a8-494c-907e-27e28e7ae2f8",
   "metadata": {},
   "source": [
    "## Readme\n",
    "\n",
    "This Jupyter Notebook analyzes the correlation among relevant  numerical features in a dataset to identify and remove highly correlated features. The purpose of this analysis is to reduce multicollinearity, which can negatively impact the performance of machine learning models.\n",
    "\n",
    "From the 20 relevant features analyzed, the following 8 features are retained for further use due to their lower correlation:\n",
    "- 'reading_time'\n",
    "- 'mistakes_dist_ratio'\n",
    "- 'polysyllabcount'\n",
    "- 'sentence_count'\n",
    "- 'difficult_words'\n",
    "- 'comma_count'\n",
    "- 'transitional_phrases_c'\n",
    "- 'text_dist_words_ratio'\n",
    "\n",
    "Other features were excluded due to high correlation (greater than 0.9). The Pearson correlation coefficient was used for the correlation analysis.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9840fa01-8287-4bac-a30e-72b41fd91d34",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [1. Import Libraries](#1-Import-Libraries)\n",
    "- [2. Load Data](#2-Load-Data)\n",
    "- [3. Correlation Analysis](#3-Correlation-Analysis)\n",
    "- [4. Analyze Manually What to Remove](#4-Analyze-Manually-What-to-Remove)\n",
    "- [5. Repeat Correlation Analysis for Remaining Features](#5-Repeat-Correlation-Analysis-for-Remaining-Features)\n",
    "- [6. List of Remaining Features](#43-List-of-Remaining-Features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38070fc0-7caa-4da4-8fe4-c735e77ca2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2. Load Data\n",
    "df = pd.read_csv('numeric_features_added_v1.csv')\n",
    "\n",
    "# List of relevant features\n",
    "relevant_features = [\n",
    "    'syllable_count', 'letters_count', 'char_count', 'reading_time_minutes', 'reading_time', \n",
    "    'preprocessed_text_count', 'lexicon_count', 'word_count_in_full_text', 'stopword_count_in_full_text', \n",
    "    'monosyllabcount', 'mistakes_dist_ratio', 'polysyllabcount', 'preprocessed_text_dist_count', 'sentence_count', \n",
    "    'mistakes_dist_dist_ratio', 'difficult_words', 'comma_count', 'transitional_phrases_c', 'transitional_phrases_dist_c', \n",
    "    'text_dist_words_ratio'\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "relevant_features_df = pd.DataFrame(relevant_features, columns=['Relevant Features'])\n",
    "\n",
    "# Filter the original DataFrame to only include relevant features\n",
    "relevant_features_data = df[relevant_features]\n",
    "\n",
    "# 4. Correlation Analysis\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = relevant_features_data.corr()\n",
    "\n",
    "# Unstack the correlation matrix to get pairs of features and their correlation\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "\n",
    "# Convert the series to a DataFrame and reset the index\n",
    "corr_pairs_df = pd.DataFrame(corr_pairs, columns=['Correlation']).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "corr_pairs_df.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "\n",
    "# Remove self-correlations (where Feature1 == Feature2)\n",
    "corr_pairs_df = corr_pairs_df[corr_pairs_df['Feature1'] != corr_pairs_df['Feature2']]\n",
    "\n",
    "# Drop duplicate pairs (e.g., (A, B) and (B, A))\n",
    "corr_pairs_df['Pair'] = corr_pairs_df.apply(lambda row: tuple(sorted([row['Feature1'], row['Feature2']])), axis=1)\n",
    "corr_pairs_df = corr_pairs_df.drop_duplicates(subset='Pair').drop(columns='Pair')\n",
    "\n",
    "# Sort the correlations in descending order\n",
    "sorted_corr_pairs_df = corr_pairs_df.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Suggest features for removal with correlation >= 0.90\n",
    "high_corr_features = sorted_corr_pairs_df[sorted_corr_pairs_df['Correlation'] >= 0.90]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a3d3c2-1a85-42aa-bed8-a00e5367899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>reading_time</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>char_count</td>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>char_count</td>\n",
       "      <td>reading_time</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>transitional_phrases_dist_c</td>\n",
       "      <td>0.999887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>letters_count</td>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>0.999829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>letters_count</td>\n",
       "      <td>reading_time</td>\n",
       "      <td>0.999829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>letters_count</td>\n",
       "      <td>char_count</td>\n",
       "      <td>0.999829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.999818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>0.999419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.999208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>reading_time</td>\n",
       "      <td>0.998288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>0.998288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>char_count</td>\n",
       "      <td>0.998288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>letters_count</td>\n",
       "      <td>0.998266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>letters_count</td>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>0.987069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>char_count</td>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>0.986895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>0.986894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>0.986894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>letters_count</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.986885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>char_count</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.986723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.986723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.986723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>lexicon_count</td>\n",
       "      <td>0.986643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>word_count_in_full_text</td>\n",
       "      <td>0.986439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>letters_count</td>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>0.986049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>char_count</td>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>0.985679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>0.985679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>reading_time_minutes</td>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>0.985679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>syllable_count</td>\n",
       "      <td>preprocessed_text_count</td>\n",
       "      <td>0.985354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>stopword_count_in_full_text</td>\n",
       "      <td>monosyllabcount</td>\n",
       "      <td>0.983167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Feature1                     Feature2  Correlation\n",
       "64          reading_time_minutes                 reading_time     1.000000\n",
       "43                    char_count         reading_time_minutes     1.000000\n",
       "44                    char_count                 reading_time     1.000000\n",
       "358       transitional_phrases_c  transitional_phrases_dist_c     0.999887\n",
       "23                 letters_count         reading_time_minutes     0.999829\n",
       "24                 letters_count                 reading_time     0.999829\n",
       "22                 letters_count                   char_count     0.999829\n",
       "127                lexicon_count      word_count_in_full_text     0.999818\n",
       "106      preprocessed_text_count                lexicon_count     0.999419\n",
       "107      preprocessed_text_count      word_count_in_full_text     0.999208\n",
       "4                 syllable_count                 reading_time     0.998288\n",
       "3                 syllable_count         reading_time_minutes     0.998288\n",
       "2                 syllable_count                   char_count     0.998288\n",
       "1                 syllable_count                letters_count     0.998266\n",
       "26                 letters_count                lexicon_count     0.987069\n",
       "46                    char_count                lexicon_count     0.986895\n",
       "86                  reading_time                lexicon_count     0.986894\n",
       "66          reading_time_minutes                lexicon_count     0.986894\n",
       "27                 letters_count      word_count_in_full_text     0.986885\n",
       "47                    char_count      word_count_in_full_text     0.986723\n",
       "87                  reading_time      word_count_in_full_text     0.986723\n",
       "67          reading_time_minutes      word_count_in_full_text     0.986723\n",
       "6                 syllable_count                lexicon_count     0.986643\n",
       "7                 syllable_count      word_count_in_full_text     0.986439\n",
       "25                 letters_count      preprocessed_text_count     0.986049\n",
       "45                    char_count      preprocessed_text_count     0.985679\n",
       "85                  reading_time      preprocessed_text_count     0.985679\n",
       "65          reading_time_minutes      preprocessed_text_count     0.985679\n",
       "5                 syllable_count      preprocessed_text_count     0.985354\n",
       "169  stopword_count_in_full_text              monosyllabcount     0.983167"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_corr_pairs_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b88ccf-32e9-4caf-b254-6f95a56ddb62",
   "metadata": {},
   "source": [
    "### Analyze manually what to remove (1 out of 2 correlated should be left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41cc818e-f184-4a8e-ba47-de792e99c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = [\n",
    "    'reading_time_minutes',\n",
    "    'char_count',\n",
    "    'letters_count',\n",
    "    'syllable_count',\n",
    "    'lexicon_count',\n",
    "    'word_count_in_full_text',\n",
    "    'transitional_phrases_dist_c',\n",
    "    'mistakes_dist_dist_ratio',\n",
    "    'preprocessed_text_dist_count',\n",
    "    'monosyllabcount',\n",
    "    'preprocessed_text_count',\n",
    "    'stopword_count_in_full_text'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8f0d9b-8ef1-4432-a08c-a0b3794a913a",
   "metadata": {},
   "source": [
    "### Repeat correlation analysis for remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d35b786-a349-4b5c-8e57-e08f36a3b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2. Load Data\n",
    "df = pd.read_csv('numeric_features_added_v1.csv')\n",
    "\n",
    "# List of relevant features\n",
    "relevant_features = [\n",
    "    'reading_time',\n",
    "    'mistakes_dist_ratio',\n",
    "    'polysyllabcount', 'sentence_count',\n",
    "    'difficult_words', 'comma_count',\n",
    "    'transitional_phrases_c', 'text_dist_words_ratio'\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "relevant_features_df = pd.DataFrame(relevant_features, columns=['Relevant Features'])\n",
    "\n",
    "# Filter the original DataFrame to only include relevant features\n",
    "relevant_features_data = df[relevant_features]\n",
    "\n",
    "# 4. Correlation Analysis\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = relevant_features_data.corr()\n",
    "\n",
    "# Unstack the correlation matrix to get pairs of features and their correlation\n",
    "corr_pairs = corr_matrix.unstack()\n",
    "\n",
    "# Convert the series to a DataFrame and reset the index\n",
    "corr_pairs_df = pd.DataFrame(corr_pairs, columns=['Correlation']).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "corr_pairs_df.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "\n",
    "# Remove self-correlations (where Feature1 == Feature2)\n",
    "corr_pairs_df = corr_pairs_df[corr_pairs_df['Feature1'] != corr_pairs_df['Feature2']]\n",
    "\n",
    "# Drop duplicate pairs (e.g., (A, B) and (B, A))\n",
    "corr_pairs_df['Pair'] = corr_pairs_df.apply(lambda row: tuple(sorted([row['Feature1'], row['Feature2']])), axis=1)\n",
    "corr_pairs_df = corr_pairs_df.drop_duplicates(subset='Pair').drop(columns='Pair')\n",
    "\n",
    "# Sort the correlations in descending order\n",
    "sorted_corr_pairs_df = corr_pairs_df.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Suggest features for removal with correlation >= 0.91\n",
    "high_corr_features = sorted_corr_pairs_df[sorted_corr_pairs_df['Correlation'] >= 0.90]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff14669-ea41-41ad-b9c4-48ed6c3a41e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>difficult_words</td>\n",
       "      <td>0.896468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>0.859977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>difficult_words</td>\n",
       "      <td>0.846999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>sentence_count</td>\n",
       "      <td>0.801289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>comma_count</td>\n",
       "      <td>0.703225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>0.702323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>comma_count</td>\n",
       "      <td>0.676264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>comma_count</td>\n",
       "      <td>0.675047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>0.664977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>sentence_count</td>\n",
       "      <td>0.639405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sentence_count</td>\n",
       "      <td>difficult_words</td>\n",
       "      <td>0.627431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>0.582365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>0.565224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sentence_count</td>\n",
       "      <td>comma_count</td>\n",
       "      <td>0.561753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>comma_count</td>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>0.537805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sentence_count</td>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>0.531022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>difficult_words</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>-0.190517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>comma_count</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>-0.278753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>-0.349242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>-0.355918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sentence_count</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>-0.510724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>comma_count</td>\n",
       "      <td>-0.510966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>difficult_words</td>\n",
       "      <td>-0.552679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>transitional_phrases_c</td>\n",
       "      <td>-0.565382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>text_dist_words_ratio</td>\n",
       "      <td>-0.587452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>polysyllabcount</td>\n",
       "      <td>-0.598631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>sentence_count</td>\n",
       "      <td>-0.669639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reading_time</td>\n",
       "      <td>mistakes_dist_ratio</td>\n",
       "      <td>-0.777761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature1                Feature2  Correlation\n",
       "20         polysyllabcount         difficult_words     0.896468\n",
       "2             reading_time         polysyllabcount     0.859977\n",
       "4             reading_time         difficult_words     0.846999\n",
       "3             reading_time          sentence_count     0.801289\n",
       "5             reading_time             comma_count     0.703225\n",
       "15     mistakes_dist_ratio   text_dist_words_ratio     0.702323\n",
       "21         polysyllabcount             comma_count     0.676264\n",
       "37         difficult_words             comma_count     0.675047\n",
       "6             reading_time  transitional_phrases_c     0.664977\n",
       "19         polysyllabcount          sentence_count     0.639405\n",
       "28          sentence_count         difficult_words     0.627431\n",
       "38         difficult_words  transitional_phrases_c     0.582365\n",
       "22         polysyllabcount  transitional_phrases_c     0.565224\n",
       "29          sentence_count             comma_count     0.561753\n",
       "46             comma_count  transitional_phrases_c     0.537805\n",
       "30          sentence_count  transitional_phrases_c     0.531022\n",
       "39         difficult_words   text_dist_words_ratio    -0.190517\n",
       "47             comma_count   text_dist_words_ratio    -0.278753\n",
       "55  transitional_phrases_c   text_dist_words_ratio    -0.349242\n",
       "23         polysyllabcount   text_dist_words_ratio    -0.355918\n",
       "31          sentence_count   text_dist_words_ratio    -0.510724\n",
       "13     mistakes_dist_ratio             comma_count    -0.510966\n",
       "12     mistakes_dist_ratio         difficult_words    -0.552679\n",
       "14     mistakes_dist_ratio  transitional_phrases_c    -0.565382\n",
       "7             reading_time   text_dist_words_ratio    -0.587452\n",
       "10     mistakes_dist_ratio         polysyllabcount    -0.598631\n",
       "11     mistakes_dist_ratio          sentence_count    -0.669639\n",
       "1             reading_time     mistakes_dist_ratio    -0.777761"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_corr_pairs_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0af79-fcd0-405a-a9e6-ba4cc7a8cbaa",
   "metadata": {},
   "source": [
    "### List of remaining features (with correlation < 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616f7682-45f7-46b7-a055-5c314048db71",
   "metadata": {},
   "source": [
    "From the 20 ralevant numerical features, the following 8 will be retained for further use. The rest are excluded due to high correlation (greater than 0.9):\n",
    "\n",
    "- 'reading_time'\n",
    "- 'mistakes_dist_ratio'\n",
    "- 'polysyllabcount'\n",
    "- 'sentence_count'\n",
    "- 'difficult_words'\n",
    "- 'comma_count'\n",
    "- 'transitional_phrases_c'\n",
    "- 'text_dist_words_ratio'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b582100-79d8-4387-98cb-03f224f2547a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
