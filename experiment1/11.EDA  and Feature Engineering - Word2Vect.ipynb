{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20c3b0f-8605-4927-891b-846bc5869aa8",
   "metadata": {},
   "source": [
    "- To apply Word2Vec for predicting essay scores, we'll use the gensim library to create word embeddings. \n",
    "Given that we are using lemmatized text without stop words, punctuation, and special characters for Word2Vec embeddings, this preprocessing is typically beneficial as Word2Vec focuses on learning the semantic meaning and relationships between words.\n",
    "- For Word2Vec, it is generally recommended to convert text to lowercase before training the model. This step ensures that words like \"Apple\" and \"apple\" are treated as the same word, which helps in reducing the vocabulary size and improves the model's ability to learn meaningful representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc25ad7d-83fb-40e4-b158-1adcdba679a4",
   "metadata": {},
   "source": [
    "### Content\n",
    "  - [1. Get Data](#1-Get-Data)\n",
    "  - [2. Analyze Text Length]()\n",
    "  - [3. Conclusion (explanation of parameters)]()\n",
    "  - [4. Train the Word2Vec Model]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13a0d538-c942-4f74-a5ed-85ca887075b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('transformed_data_v1.csv')\n",
    "df_numerical = pd.read_csv('numeric_features_added_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b77fe-c0dd-4c77-801f-37dacb34c843",
   "metadata": {},
   "source": [
    "### 2. Analyze Text Length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc972ce1-0e8c-40a2-bbd6-7b42abec62b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum text length: 57\n",
      "Maximum text length: 840\n",
      "Average text length: 183.58469984829878\n"
     ]
    }
   ],
   "source": [
    "# Choose the text column (lemmatized text without stop words, punctuation, and special characters)\n",
    "text_column = 'clean_lemm_preprocessed_text'\n",
    "\n",
    "# Convert text to lowercase\n",
    "df[text_column] = df[text_column].str.lower()\n",
    "\n",
    "# Analyze text lengths\n",
    "df['text_length'] = df[text_column].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate minimum, maximum, and average length\n",
    "min_length = df['text_length'].min()\n",
    "max_length = df['text_length'].max()\n",
    "average_length = df['text_length'].mean()\n",
    "\n",
    "print(f'Minimum text length: {min_length}')\n",
    "print(f'Maximum text length: {max_length}')\n",
    "print(f'Average text length: {average_length}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "948d3fc4-9080-4a87-b22c-66f0aa97de1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "count    13843.000000\n",
      "mean        20.394512\n",
      "std         13.308014\n",
      "min          5.354167\n",
      "25%         15.800000\n",
      "50%         18.578947\n",
      "75%         22.200000\n",
      "max        715.000000\n",
      "Name: avg_sentence_length, dtype: float64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Vocabulary size: 54074\n"
     ]
    }
   ],
   "source": [
    "# Analyze average text lengths\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "print(df_numerical['avg_sentence_length'].describe())\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "sentences = df['clean_lemm_preprocessed_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Calculate vocabulary size\n",
    "vocab = Counter(word for sentence in sentences for word in sentence)\n",
    "vocab_size = len(vocab)\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834830f4-f454-4abb-9bef-9c19f3d9c7f2",
   "metadata": {},
   "source": [
    "### 3. Conclusion (explanation of parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa3fc5-a39c-4c09-8f71-438fab15a66e",
   "metadata": {},
   "source": [
    "Based on the above information about the dataset, here are parameters for the Word2Vec model with explained reasoning behind these choices:\n",
    "- **vector_size=300:** This larger size helps capture more semantic nuances, which is beneficial for a diverse and rich vocabulary.\n",
    "- **window=5:** A window size of 5 is sufficient to capture the context within the average sentence length of approximately 20 words.\n",
    "- **min_count=10:** Filters out infrequent words, reducing noise and focusing on more common and likely more informative words.\n",
    "- **workers=32:** Utilizes all 32 CPU cores to speed up the training process, making efficient use of the available computational resources.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3c3db-00ce-48c3-99b9-58407aad51b7",
   "metadata": {},
   "source": [
    "### 4. Train the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b74954d-981f-4492-b3ae-561898ba437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  word2vec_embedding\n",
      "0  [-0.09748275, 0.005645593, 0.33820832, 0.30875...\n",
      "1  [0.0027511492, 0.08209037, -0.15495926, 0.0202...\n",
      "2  [0.044071138, -0.09524269, 0.19774339, -0.2092...\n",
      "3  [0.0908097, -0.0639152, 0.35811678, 0.23761816...\n",
      "4  [0.034144007, -0.29616755, 0.12020076, -0.2252...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "# Choose the text column (lemmatized text without stop words)\n",
    "text_column = 'clean_lemm_preprocessed_text'\n",
    "\n",
    "# Convert text to lowercase\n",
    "df[text_column] = df[text_column].str.lower()\n",
    "\n",
    "# Prepare the data for Word2Vec\n",
    "# Tokenize the sentences into words\n",
    "sentences = df[text_column].apply(lambda x: x.split())\n",
    "\n",
    "# Train the Word2Vec model with improved parameters\n",
    "w2v_model = Word2Vec(sentences, vector_size=300, window=5, min_count=10, workers=32)\n",
    "\n",
    "# Create a function to average word vectors for a given sentence\n",
    "def average_word_vectors(sentence, model, vector_size):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Apply the function to the text column to get sentence embeddings\n",
    "vector_size = w2v_model.vector_size\n",
    "df['word2vec_embedding'] = df[text_column].apply(lambda x: average_word_vectors(x, w2v_model, vector_size))\n",
    "\n",
    "# Check the result\n",
    "print(df[['word2vec_embedding']].head())\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv('word2vec_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01714044-a47b-4e79-a8c1-16382a43d02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
