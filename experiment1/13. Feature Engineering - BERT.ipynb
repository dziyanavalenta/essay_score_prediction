{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5dd3d9-ca4d-41f9-aa8e-00bbfbda7c03",
   "metadata": {},
   "source": [
    "**BERT is designed** to understand the context and semantics of words within their original context. This includes punctuation and special characters, which can provide important context clues for BERT's attention mechanisms. Therefore, we will use the `full_text` column without lemmatization, stemming, or removal of stop words, punctuation, and special characters.\n",
    "- The code loads a dataset and preprocesses the full_text column by converting it to lowercase. \n",
    "- It then uses the bert-base-uncased model to tokenize the text, truncating and padding each to a maximum length of 512 tokens\n",
    "- Then extracts 768-dimensional embeddings from the `[CLS] token` for each text. `[CLS] Token Embeddings` (but not Full embeddings) are used in\n",
    "classification tasks such as essay scoring\n",
    "- Finally, it saves these embeddings to a new DataFrame and exports it as bert_features.csv\n",
    "\n",
    "**Loss of Information**:\n",
    "- Texts longer than 512 tokens (all with score=6 and majority with score=5) will lose the content beyond this limit, potentially leading to an incomplete representation of the text's quality and coherence, which are crucial for essay scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bef79d-c3df-46b5-9286-482cce499d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting BERT embeddings: 100%|██████████| 13843/13843 [7:27:05<00:00,  1.94s/it]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bert_feature_0  bert_feature_1  bert_feature_2  bert_feature_3  \\\n",
      "0       -0.068888       -0.556835        0.466894        0.602734   \n",
      "1       -0.490990       -0.266561        0.205436        0.357459   \n",
      "2       -0.090818       -0.227917       -0.149506        0.005978   \n",
      "3       -0.349526       -0.106863       -0.125005        0.271403   \n",
      "4        0.113317       -0.388882       -0.021579        0.849989   \n",
      "\n",
      "   bert_feature_4  bert_feature_5  bert_feature_6  bert_feature_7  \\\n",
      "0        0.041977       -0.487987       -0.031749        1.373013   \n",
      "1       -0.648571       -0.154488        0.004012        0.070033   \n",
      "2       -0.407537       -0.576724        0.743775        1.054340   \n",
      "3       -0.090610       -0.555049        0.115859        0.360005   \n",
      "4        0.051454       -0.316866        0.492008        0.901263   \n",
      "\n",
      "   bert_feature_8  bert_feature_9  ...  bert_feature_758  bert_feature_759  \\\n",
      "0       -0.359459       -0.257405  ...         -0.093191         -0.262483   \n",
      "1        0.133062       -0.821349  ...         -0.174234         -0.285230   \n",
      "2       -0.142332       -0.484320  ...         -0.492142         -0.670111   \n",
      "3        0.080597       -0.375333  ...          0.209398         -0.302860   \n",
      "4       -0.181418       -0.796159  ...          0.093533         -0.300793   \n",
      "\n",
      "   bert_feature_760  bert_feature_761  bert_feature_762  bert_feature_763  \\\n",
      "0         -0.287283         -0.109270         -0.299188          0.709983   \n",
      "1          0.371035         -0.536584          0.226327          0.289881   \n",
      "2          0.323492         -0.466708          0.134440          0.701627   \n",
      "3         -0.262703         -0.266583         -0.152692          0.594636   \n",
      "4         -0.033316         -0.218648         -0.282768          0.579345   \n",
      "\n",
      "   bert_feature_764  bert_feature_765  bert_feature_766  bert_feature_767  \n",
      "0         -0.316100         -0.824562          0.944580         -0.145644  \n",
      "1         -0.010437         -0.372561          0.897469          0.537665  \n",
      "2         -0.056288         -0.519546          0.626605         -0.148193  \n",
      "3         -0.065607         -0.435155          0.792222         -0.016925  \n",
      "4         -0.123729         -0.180977          0.363569          0.260930  \n",
      "\n",
      "[5 rows x 768 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('transformed_data_v1.csv')\n",
    "\n",
    "# Use the full_text column\n",
    "text_column = 'full_text'\n",
    "\n",
    "# Convert text to lowercase\n",
    "df[text_column] = df[text_column].str.lower()\n",
    "\n",
    "# Choose the BERT model (uncased for this example)\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get BERT embeddings\n",
    "def get_bert_embedding(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the [CLS] token representation for the embedding (768-dimensional vector)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_embedding\n",
    "\n",
    "# Apply the function to get BERT embeddings for the dataset\n",
    "embeddings = []\n",
    "\n",
    "for text in tqdm(df[text_column].tolist(), desc=\"Getting BERT embeddings\"):\n",
    "    embedding = get_bert_embedding(text, tokenizer, model)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Convert embeddings to DataFrame\n",
    "bert_embeddings_df = pd.DataFrame(embeddings, columns=[f'bert_feature_{i}' for i in range(embeddings[0].shape[0])])\n",
    "\n",
    "# Save the BERT embeddings DataFrame\n",
    "bert_embeddings_df.to_csv('bert_features.csv', index=False)\n",
    "\n",
    "# Check the result\n",
    "print(bert_embeddings_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49900e7-d8c1-48c2-93e0-e1ab2ec54b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
