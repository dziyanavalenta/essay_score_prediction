{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151ed0c5-513e-4f65-bc79-a6920de7a017",
   "metadata": {},
   "source": [
    "## README\r\n",
    "This Jupyter Notebook performs the following tasks:\r\n",
    "\r\n",
    "1. **Import Libraries**: Imports necessary libraries for data manipulation, text processing, and multiprocessing.\r\n",
    "   \r\n",
    "2. **Get Data**: Reads the input data from a CSV file.\r\n",
    "\r\n",
    "3. **Remove Specific Texts**: Removes texts with specific indexes from the  (found duplicates during analysis)dataset.\r\n",
    "   \r\n",
    "4. **Transform Data**:\r\n",
    "   - **Standardizing Contractions**: Expands common contractions using a predefined dictionary.\r\n",
    "   - **Removing HTML Tags**: Eliminates HTML tags to retain only plain text.\r\n",
    "   - **Removing Special Characters and Punctuation**: Cleanses text by removing special characters and punctuation.\r\n",
    "   - **Removing Words with Numbers**: Removes words containing numbers and any trailing 's.\r\n",
    "   - **Removing Stop Words**: Eliminates common stop words.\r\n",
    "   - **Removing Non-ASCII Characters**: Removes non-ASCII characters, including emojis.\r\n",
    "   - **Tokenization and Lemmatization**: Applies NLTK's tokenization and lemmatization using WordNet POS tags.\r\n",
    "   - **Identifying Misspelt Words**: Identifies and counts misspelled words using a spell checker.\r\n",
    "   \r\n",
    "5. **Export Data**: Exports the processed data to a CSV file.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cde7b5c-7d8b-4cce-9a31-709abc1db7fe",
   "metadata": {},
   "source": [
    "## Content\r\n",
    "  - [1. Import Libraries](#1-Import-Libraries)\r\n",
    "  - [2. Get Data](#2-Get-Data)\r\n",
    "  - [3. Remove Specific Texts](#3-Remove-Specific-Texts)\r\n",
    "  - [4. Transform Data](#4-Transform-Data)\r\n",
    "    - [Standardizing Contractions](#Standardizing-Contractions)\r\n",
    "    - [Removing HTML Tags](#Removing-HTML-Tags)\r\n",
    "    - [Removing Special Characters and Punctuation](#Removing-Special-Characters-and-Punctuation)\r\n",
    "    - [Removing Words with Numbers](#Removing-Words-with-Numbers)\r\n",
    "    - [Removing Stop Words](#Removing-Stop-Words)\r\n",
    "    - [Removing Non-ASCII Characters](#Removing-Non-ASCII-Characters)\r\n",
    "    - [Tokenization and Lemmatization](#Tokenization-and-Lemmatization)\r\n",
    "    - [Identifying Misspelt Words](#Identifying-Misspelt-Words)\r\n",
    "  - [5. Export Data](#5-Export-Data)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303688c1-4d93-45b1-a07d-a5bc81692999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 32\n",
      "Working on Preprocessed Text Features\n",
      "Elapsed time: 351.0896408557892 seconds\n",
      "Working on Misspelled Features\n",
      "Elapsed time: 160.21734404563904 seconds\n",
      "Exporting file\n",
      "File exported\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import time\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import textstat\n",
    "\n",
    "# Get the number of available CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of available CPU cores: {num_cores}\")\n",
    "\n",
    "# 2. Get Data\n",
    "df = pd.read_csv('train_split.csv')\n",
    "\n",
    "#3. Remove texts with specific indexes\n",
    "indexes_to_remove = [789, 3109] # indexes were found in duplicates Notebook\n",
    "df = df.drop(indexes_to_remove).reset_index(drop=True)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"aren't\": \"are not\", \"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\", \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\", \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"won't\": \"will not\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\", \"let's\": \"let us\",\n",
    "    \"here's\": \"here is\", \"how's\": \"how is\",\n",
    "    \"Aren't\": \"Are not\", \"Can't\": \"Cannot\", \"Could've\": \"Could have\", \"Couldn't\": \"Could not\", \"Didn't\": \"Did not\",\n",
    "    \"Doesn't\": \"Does not\", \"Don't\": \"Do not\", \"Hadn't\": \"Had not\", \"Hasn't\": \"Has not\", \"Haven't\": \"Have not\",\n",
    "    \"He'd\": \"He would\", \"He'll\": \"He will\", \"He's\": \"He is\", \"I'd\": \"I would\", \"I'll\": \"I will\", \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\", \"Isn't\": \"Is not\", \"It'd\": \"It would\", \"It'll\": \"It will\", \"It's\": \"It is\", \"Might've\": \"Might have\",\n",
    "    \"Mightn't\": \"Might not\", \"Must've\": \"Must have\", \"Mustn't\": \"Must not\", \"Shan't\": \"Shall not\", \"She'd\": \"She would\",\n",
    "    \"She'll\": \"She will\", \"She's\": \"She is\", \"Should've\": \"Should have\", \"Shouldn't\": \"Should not\", \"That'd\": \"That would\",\n",
    "    \"That's\": \"That is\", \"There's\": \"There is\", \"They'd\": \"They would\", \"They'll\": \"They will\", \"They're\": \"They are\",\n",
    "    \"They've\": \"They have\", \"Wasn't\": \"Was not\", \"We'd\": \"We would\", \"We'll\": \"We will\", \"We're\": \"We are\",\n",
    "    \"We've\": \"We have\", \"Weren't\": \"Were not\", \"What'll\": \"What will\", \"What're\": \"What are\", \"What's\": \"What is\",\n",
    "    \"What've\": \"What have\", \"Where's\": \"Where is\", \"Who'd\": \"Who would\", \"Who'll\": \"Who will\", \"Who're\": \"Who are\",\n",
    "    \"Who's\": \"Who is\", \"Who've\": \"Who have\", \"Won't\": \"Will not\", \"Would've\": \"Would have\", \"Wouldn't\": \"Would not\",\n",
    "    \"You'd\": \"You would\", \"You'll\": \"You will\", \"You're\": \"You are\", \"You've\": \"You have\", \"Let's\": \"Let us\",\n",
    "    \"Here's\": \"Here is\", \"How's\": \"How is\"\n",
    "}\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    words_and_tags = pos_tag(word_tokenize(text))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in words_and_tags]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def removeHTML(x):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', x)\n",
    "\n",
    "# Function to replace punctuation with space if absent\n",
    "def replace_punctuation_with_space_if_absent(text):\n",
    "    pattern = r'([.,!?;:]+)(?!\\s)'\n",
    "    corrected_text = re.sub(pattern, r'\\1 ', text)\n",
    "    return corrected_text\n",
    "\n",
    "# Function to replace contractions\n",
    "def replace_contractions(text, contractions_dict):\n",
    "    contractions_re = re.compile('|'.join(map(re.escape, contractions_dict.keys())))\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Function to remove trailing 's\n",
    "def remove_trailing_s(text):\n",
    "    words = text.split()\n",
    "    words = [word[:-2] if word.endswith(\"'s\") else word for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to remove special characters and punctuation\n",
    "def remove_special_characters_and_punctuation(text):\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', normalized_text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to replace multiple spaces with a single space\n",
    "def replace_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Function to remove words with numbers\n",
    "def remove_words_with_numbers(text):\n",
    "    cleaned_text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "# Function to find transitional phrases in text\n",
    "transitional_phrases = [\n",
    "    'Above all', 'Accordingly', 'Additionally', 'After', 'After all', 'Afterward', 'All in all', 'Also', 'Alternatively', \n",
    "    'As a result', 'As an illustration', 'As long as', 'As mentioned earlier', 'As noted', 'At the same time', 'Before', \n",
    "    'Besides', 'But', 'By all means', 'Consequently', 'Conversely', 'Correspondingly', 'Despite', 'During', 'Even if', \n",
    "    'Even so', 'Especially', 'Eventually', 'Finally', 'First', 'For example', 'For instance', 'Furthermore', 'Hence', \n",
    "    'However', 'If', 'In addition', 'In brief', 'In case', 'In comparison', 'In conclusion', 'In fact', 'In contrast', \n",
    "    'In other words', 'In particular', 'In simpler terms', 'In summary', 'In the meantime', 'In the same way', 'Indeed', \n",
    "    'Instead', 'Lastly', 'Later', 'Likewise', 'Meanwhile', 'Moreover', 'More importantly', 'Namely', 'Nevertheless', \n",
    "    'Next', 'Nonetheless', 'Notably', 'Now', 'On the contrary', 'On condition that', 'On one hand', 'On the other hand', \n",
    "    'Overall', 'Particularly', 'Plus', 'Previously', 'Provided that', 'Regardless', 'Second', 'Similarly', 'Since', \n",
    "    'Specifically', 'Still', 'Subsequently', 'That is', 'Then', 'Therefore', 'Third', 'Thus', 'To clarify', 'To conclude', \n",
    "    'To demonstrate', 'To illustrate', 'To put it another way', 'To summarize', 'To sum up', 'Ultimately', 'Unless', 'Unlike', \n",
    "    'Until', 'Whereas', 'Yet', 'Above and beyond', 'According to', 'After a while', 'All things considered', 'Although', \n",
    "    'Another key point', 'As a consequence', 'As a matter of fact', 'As can be seen', 'As far as', 'As soon as', 'At first', \n",
    "    'At last', 'At length', 'At this point', 'Be that as it may', 'By and large', 'By the same token', 'Even though', \n",
    "    'For fear that', 'For that reason', 'For the most part', 'Granted', 'Henceforth', 'If by chance', 'If so', 'In a moment', \n",
    "    'In any case', 'In any event', 'In light of', 'In order to', 'In particular', 'In reality', 'In short', 'In spite of', \n",
    "    'In view of', 'It follows that', 'Least of all', 'Most importantly', 'Needless to say', 'Of course', 'On the whole', \n",
    "    'One example is', 'One reason is', 'Or', 'Over time', 'Prior to', 'Provided that', 'Seeing that', 'So as to', 'Sooner or later', \n",
    "    'Such as', 'That being said', 'The next step', 'Thereafter', 'Thereby', 'Thirdly', 'Through', 'Till', 'To be sure', \n",
    "    'To begin with', 'To illustrate', 'To reiterate', 'To the end that', 'To this end', 'Until now', 'Up to now', 'What is more', \n",
    "    'Without a doubt', 'Without delay', 'Without exception', 'Yet again'\n",
    "]\n",
    "\n",
    "def find_transitional_phrases(text):\n",
    "    return [phrase for phrase in transitional_phrases if phrase.lower() in text.lower()]\n",
    "\n",
    "def preprocessed_text_part1(text):\n",
    "    text = removeHTML(text)\n",
    "    text = re.sub(\"@\\w+\", '', text)\n",
    "    text = re.sub(r\"\\b\\d+(?:'s?)?\\b\", '', text)\n",
    "    text = re.sub(\"http\\w+\", '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = replace_punctuation_with_space_if_absent(text)\n",
    "    text = replace_contractions(text, contractions)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    text = remove_trailing_s(text)\n",
    "    text = remove_special_characters_and_punctuation(text)\n",
    "    text = replace_multiple_spaces(text)\n",
    "    text = remove_words_with_numbers(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_text_from_emojis_and_non_ascii(text):\n",
    "    text = re.sub(r'[^\\w\\s,.\\n]', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return re.sub(r'\\s+', ' ', text).replace('\\n ', '\\n')\n",
    "\n",
    "def find_misspelled_words(text):\n",
    "    spell = SpellChecker()\n",
    "    words = word_tokenize(text)\n",
    "    misspelled_words = spell.unknown(words)\n",
    "    return [word for word in misspelled_words if re.match(r'\\w+', word)]\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# 4. Transform data\n",
    "\n",
    "# Preprocessed Text\n",
    "print(\"Working on Preprocessed Text Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['preprocessed_text_part1'] = Parallel(n_jobs=num_cores)(delayed(preprocessed_text_part1)(row) for row in df['full_text'])\n",
    "df['preprocessed_text'] = Parallel(n_jobs=num_cores)(delayed(preprocessed_text)(row) for row in df['preprocessed_text_part1'])\n",
    "df['lemmatized_preprocessed_text'] = df['preprocessed_text'].apply(lemmatize_text)\n",
    "df['clean_lemm_preprocessed_text'] = df['lemmatized_preprocessed_text'].apply(remove_stop_words)\n",
    "df['full_text_without_non_ascii'] = Parallel(n_jobs=num_cores)(delayed(clean_text_from_emojis_and_non_ascii)(row) for row in df['full_text'])\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Misspelled Words\n",
    "print(\"Working on Misspelled Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "df['misspelled_words_spell_checker'] = Parallel(n_jobs=num_cores)(delayed(find_misspelled_words)(row) for row in df['preprocessed_text'])\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "# 5. Export Data\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "print(\"Exporting file\")\n",
    "\n",
    "df.to_csv('transformed_data_exp_2.csv', index=False)\n",
    "print(\"File exported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97166f7-c049-4063-b8c3-46f0993f6dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
