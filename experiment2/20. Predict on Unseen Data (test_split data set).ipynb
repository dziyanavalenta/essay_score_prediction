{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f034e516-7874-4a39-b121-2fd1d6917a19",
   "metadata": {},
   "source": [
    "# Prediction on Unseen Data\n",
    "\n",
    "## 1. Imports\n",
    "\n",
    "## 2. Load Data\n",
    "- Load Test Data\n",
    "- Load Pre-trained Scaler, Model, and TF-IDF Vectorizer\n",
    "\n",
    "## 3. Preprocess Data\n",
    "### 3.1 Define Preprocessing Functions\n",
    "- HTML Removal\n",
    "- Punctuation Replacement\n",
    "- Contraction Replacement\n",
    "- Trailing 's Removal\n",
    "- Special Character and Punctuation Removal\n",
    "- Multiple Spaces Replacement\n",
    "- Words with Numbers Removal\n",
    "- Lemmatization\n",
    "- Emoji and Non-ASCII Character Removal\n",
    "- Stop Words Removal\n",
    "- Misspelled Words Identification\n",
    "\n",
    "### 3.2 Apply Preprocessing Functions\n",
    "- Transformation\n",
    "- Scaling Numerical Features\n",
    "- TF-IDF Vectorizer and PCA\n",
    "- Combine Numerical and TF-IDF Features\n",
    "\n",
    "## 4. Predicting and Discretizing Test Data via CatBoost Regressor\n",
    "- Ensure Target Classes Consistency\n",
    "- Predict on Test Data\n",
    "- Discretize Predictions\n",
    "\n",
    "## 5. Evaluate Results on Unseen Data Set\n",
    "- Calculate QWK Score\n",
    "\n",
    "## 6. Traceability Matrix and Detailed Analysis\n",
    "- Confusion Matrix\n",
    "- Detailed Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feccd5e3-a01d-4262-aeea-3ecdb84f1b91",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc30b48-1fee-4989-b7f5-10e1775aa209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPU cores: 32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import time\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "# import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import textstat\n",
    "# from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.metrics')\n",
    "\n",
    "# Get the number of available CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of available CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7d054-a2d2-452f-b9cd-53cf7fd6cebf",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5ecacf2-1463-4452-b5ca-6f72c0c7d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "test_df = pd.read_csv('test_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32345621-46a7-4fe1-a25a-4ab1ae92a23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4514, 21)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c66b7-8a99-4467-a3cd-f25f21bd5bd7",
   "metadata": {},
   "source": [
    "# Load pre-trained scaler, TF-IDF vectorizer, PCA, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a40ce3c7-83c6-4d9a-95a2-0e15ad79bc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded\n",
      "TF-IDF Vectorizer loaded\n",
      "PCA loaded\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from joblib import load\n",
    "\n",
    "# Load the pre-trained scaler\n",
    "scaler = joblib.load('scaler_exp2.pkl')\n",
    "print(\"Scaler loaded\")\n",
    "\n",
    "# Load the pre-trained TF-IDF vectorizer\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer_exp2.pkl')\n",
    "print(\"TF-IDF Vectorizer loaded\")\n",
    "\n",
    "# Load the pre-trained PCA\n",
    "pca = joblib.load('pca_500_exp2.pkl')\n",
    "print(\"PCA loaded\")\n",
    "\n",
    "# Load pre-trained model\n",
    "model_path = 'catboost_model_exp2_pca_500.pkl'\n",
    "cb_reg = load(model_path)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d3f65-ad12-4bf1-bd2e-989255e838b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "493297e4-c45e-4b9f-a467-55afdb5b167e",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684368d-0863-46c2-adbc-22db4f786a0d",
   "metadata": {},
   "source": [
    "### Define preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06822a22-f368-454f-a539-268ae245c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Contractions dictionary\n",
    "contractions = {\n",
    "    \"aren't\": \"are not\", \"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\", \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\", \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\", \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\", \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"won't\": \"will not\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\", \"let's\": \"let us\",\n",
    "    \"here's\": \"here is\", \"how's\": \"how is\",\n",
    "    \"Aren't\": \"Are not\", \"Can't\": \"Cannot\", \"Could've\": \"Could have\", \"Couldn't\": \"Could not\", \"Didn't\": \"Did not\",\n",
    "    \"Doesn't\": \"Does not\", \"Don't\": \"Do not\", \"Hadn't\": \"Had not\", \"Hasn't\": \"Has not\", \"Haven't\": \"Have not\",\n",
    "    \"He'd\": \"He would\", \"He'll\": \"He will\", \"He's\": \"He is\", \"I'd\": \"I would\", \"I'll\": \"I will\", \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\", \"Isn't\": \"Is not\", \"It'd\": \"It would\", \"It'll\": \"It will\", \"It's\": \"It is\", \"Might've\": \"Might have\",\n",
    "    \"Mightn't\": \"Might not\", \"Must've\": \"Must have\", \"Mustn't\": \"Must not\", \"Shan't\": \"Shall not\", \"She'd\": \"She would\",\n",
    "    \"She'll\": \"She will\", \"She's\": \"She is\", \"Should've\": \"Should have\", \"Shouldn't\": \"Should not\", \"That'd\": \"That would\",\n",
    "    \"That's\": \"That is\", \"There's\": \"There is\", \"They'd\": \"They would\", \"They'll\": \"They will\", \"They're\": \"They are\",\n",
    "    \"They've\": \"They have\", \"Wasn't\": \"Was not\", \"We'd\": \"We would\", \"We'll\": \"We will\", \"We're\": \"We are\",\n",
    "    \"We've\": \"We have\", \"Weren't\": \"Were not\", \"What'll\": \"What will\", \"What're\": \"What are\", \"What's\": \"What is\",\n",
    "    \"What've\": \"What have\", \"Where's\": \"Where is\", \"Who'd\": \"Who would\", \"Who'll\": \"Who will\", \"Who're\": \"Who are\",\n",
    "    \"Who's\": \"Who is\", \"Who've\": \"Who have\", \"Won't\": \"Will not\", \"Would've\": \"Would have\", \"Wouldn't\": \"Would not\",\n",
    "    \"You'd\": \"You would\", \"You'll\": \"You will\", \"You're\": \"You are\", \"You've\": \"You have\", \"Let's\": \"Let us\",\n",
    "    \"Here's\": \"Here is\", \"How's\": \"How is\"\n",
    "}\n",
    "\n",
    "transitional_phrases = [\n",
    "    'Above all', 'Accordingly', 'Additionally', 'After', 'After all', 'Afterward', 'All in all', 'Also', 'Alternatively', \n",
    "    'As a result', 'As an illustration', 'As long as', 'As mentioned earlier', 'As noted', 'At the same time', 'Before', \n",
    "    'Besides', 'But', 'By all means', 'Consequently', 'Conversely', 'Correspondingly', 'Despite', 'During', 'Even if', \n",
    "    'Even so', 'Especially', 'Eventually', 'Finally', 'First', 'For example', 'For instance', 'Furthermore', 'Hence', \n",
    "    'However', 'If', 'In addition', 'In brief', 'In case', 'In comparison', 'In conclusion', 'In fact', 'In contrast', \n",
    "    'In other words', 'In particular', 'In simpler terms', 'In summary', 'In the meantime', 'In the same way', 'Indeed', \n",
    "    'Instead', 'Lastly', 'Later', 'Likewise', 'Meanwhile', 'Moreover', 'More importantly', 'Namely', 'Nevertheless', \n",
    "    'Next', 'Nonetheless', 'Notably', 'Now', 'On the contrary', 'On condition that', 'On one hand', 'On the other hand', \n",
    "    'Overall', 'Particularly', 'Plus', 'Previously', 'Provided that', 'Regardless', 'Second', 'Similarly', 'Since', \n",
    "    'Specifically', 'Still', 'Subsequently', 'That is', 'Then', 'Therefore', 'Third', 'Thus', 'To clarify', 'To conclude', \n",
    "    'To demonstrate', 'To illustrate', 'To put it another way', 'To summarize', 'To sum up', 'Ultimately', 'Unless', 'Unlike', \n",
    "    'Until', 'Whereas', 'Yet', 'Above and beyond', 'According to', 'After a while', 'All things considered', 'Although', \n",
    "    'Another key point', 'As a consequence', 'As a matter of fact', 'As can be seen', 'As far as', 'As soon as', 'At first', \n",
    "    'At last', 'At length', 'At this point', 'Be that as it may', 'By and large', 'By the same token', 'Even though', \n",
    "    'For fear that', 'For that reason', 'For the most part', 'Granted', 'Henceforth', 'If by chance', 'If so', 'In a moment', \n",
    "    'In any case', 'In any event', 'In light of', 'In order to', 'In particular', 'In reality', 'In short', 'In spite of', \n",
    "    'In view of', 'It follows that', 'Least of all', 'Most importantly', 'Needless to say', 'Of course', 'On the whole', \n",
    "    'One example is', 'One reason is', 'Or', 'Over time', 'Prior to', 'Provided that', 'Seeing that', 'So as to', 'Sooner or later', \n",
    "    'Such as', 'That being said', 'The next step', 'Thereafter', 'Thereby', 'Thirdly', 'Through', 'Till', 'To be sure', \n",
    "    'To begin with', 'To illustrate', 'To reiterate', 'To the end that', 'To this end', 'Until now', 'Up to now', 'What is more', \n",
    "    'Without a doubt', 'Without delay', 'Without exception', 'Yet again'\n",
    "]\n",
    "\n",
    "# Function to convert NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    words_and_tags = pos_tag(word_tokenize(text))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in words_and_tags]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Function to remove HTML tags\n",
    "def removeHTML(x):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', x)\n",
    "\n",
    "# Function to replace punctuation with space if absent\n",
    "def replace_punctuation_with_space_if_absent(text):\n",
    "    pattern = r'([.,!?;:]+)(?!\\s)'\n",
    "    corrected_text = re.sub(pattern, r'\\1 ', text)\n",
    "    return corrected_text\n",
    "\n",
    "# Function to replace contractions\n",
    "def replace_contractions(text, contractions_dict):\n",
    "    contractions_re = re.compile('|'.join(map(re.escape, contractions_dict.keys())))\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Function to remove trailing 's\n",
    "def remove_trailing_s(text):\n",
    "    words = text.split()\n",
    "    words = [word[:-2] if word.endswith(\"'s\") else word for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to remove special characters and punctuation\n",
    "def remove_special_characters_and_punctuation(text):\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', normalized_text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to replace multiple spaces with a single space\n",
    "def replace_multiple_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Function to remove words with numbers\n",
    "def remove_words_with_numbers(text):\n",
    "    cleaned_text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    return re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "def find_transitional_phrases(text):\n",
    "    return [phrase for phrase in transitional_phrases if phrase.lower() in text.lower()]\n",
    "\n",
    "def preprocessed_text_part1(text):\n",
    "    text = removeHTML(text)\n",
    "    text = re.sub(\"@\\w+\", '', text)\n",
    "    text = re.sub(r\"\\b\\d+(?:'s?)?\\b\", '', text)\n",
    "    text = re.sub(\"http\\w+\", '', text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\,+\", \",\", text)\n",
    "    text = replace_punctuation_with_space_if_absent(text)\n",
    "    text = replace_contractions(text, contractions)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def preprocessed_text(text):\n",
    "    text = remove_trailing_s(text)\n",
    "    text = remove_special_characters_and_punctuation(text)\n",
    "    text = replace_multiple_spaces(text)\n",
    "    text = remove_words_with_numbers(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_text_from_emojis_and_non_ascii(text):\n",
    "    text = re.sub(r'[^\\w\\s,.\\n]', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return re.sub(r'\\s+', ' ', text).replace('\\n ', '\\n')\n",
    "\n",
    "def find_misspelled_words(text):\n",
    "    spell = SpellChecker()\n",
    "    words = word_tokenize(text)\n",
    "    misspelled_words = spell.unknown(words)\n",
    "    return [word for word in misspelled_words if re.match(r'\\w+', word)]\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "def textstat_features(text):\n",
    "    features = {}   \n",
    "    features['difficult_words'] = textstat.difficult_words(text)  \n",
    "    features['reading_time'] = textstat.reading_time(text)  \n",
    "    features['sentence_count'] = textstat.sentence_count(text)  \n",
    "    features['polysyllabcount'] = textstat.polysyllabcount(text)  \n",
    "    return features  \n",
    "\n",
    "# Function to find transitional phrases in text\n",
    "def find_transitional_phrases(text):\n",
    "    return [phrase for phrase in transitional_phrases if phrase.lower() in text.lower()]\n",
    "\n",
    "# General text counting functions\n",
    "def count_phrases_in_list(phrases_list):\n",
    "    return len(phrases_list)\n",
    "\n",
    "def count_distinct_mistakes(word_list):\n",
    "    return len(set(word_list))\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_distinct_words(text):\n",
    "    words = text.lower().split()\n",
    "    return len(set(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55360aec-4b7d-4aee-b3be-09e90478e945",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "474fa562-5527-4043-a9a4-6aa9b3682d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Preprocessed Text Features\n",
      "Elapsed time: 342.83931851387024 seconds\n",
      "Working on Misspelled Features\n",
      "Elapsed time: 108.97266125679016 seconds\n",
      "Working on Text Analysis Features\n",
      "Elapsed time: 8.923282623291016 seconds\n",
      "Working on Ratio Features\n",
      "Elapsed time: 0.3555586338043213 seconds\n",
      "Working on Text Statistics Features\n",
      "Elapsed time: 8.227802515029907 seconds\n"
     ]
    }
   ],
   "source": [
    "# 4. Feature Engineering\n",
    "# Preprocessed Text\n",
    "print(\"Working on Preprocessed Text Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "test_df['preprocessed_text_part1'] = Parallel(n_jobs=num_cores)(delayed(preprocessed_text_part1)(row) for row in test_df['full_text'])\n",
    "test_df['preprocessed_text'] = Parallel(n_jobs=num_cores)(delayed(preprocessed_text)(row) for row in test_df['preprocessed_text_part1'])\n",
    "test_df['lemmatized_preprocessed_text'] = test_df['preprocessed_text'].apply(lemmatize_text)\n",
    "test_df['clean_lemm_preprocessed_text'] = test_df['lemmatized_preprocessed_text'].apply(remove_stop_words)\n",
    "test_df['full_text_without_non_ascii'] = Parallel(n_jobs=num_cores)(delayed(clean_text_from_emojis_and_non_ascii)(row) for row in test_df['full_text'])\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Misspelled Words\n",
    "print(\"Working on Misspelled Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "test_df['misspelled_words_spell_checker'] = Parallel(n_jobs=num_cores)(delayed(find_misspelled_words)(row) for row in test_df['preprocessed_text'])\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "## Text Analysis Features\n",
    "print(\"Working on Text Analysis Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "test_df['comma_count'] = test_df['full_text'].str.count(',')\n",
    "test_df['transitional_phrases'] = test_df['preprocessed_text'].apply(find_transitional_phrases)\n",
    "test_df['mistakes_dist_count'] = test_df['misspelled_words_spell_checker'].apply(count_distinct_mistakes)\n",
    "test_df['transitional_phrases_c'] = test_df['transitional_phrases'].apply(count_phrases_in_list)\n",
    "test_df['preprocessed_text_count'] = test_df['preprocessed_text'].apply(count_words)\n",
    "test_df['preprocessed_text_dist_count'] = test_df['preprocessed_text'].apply(count_distinct_words)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "## Ratio Features\n",
    "print(\"Working on Ratio Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "test_df['text_dist_words_ratio'] = test_df.apply(lambda x: x['preprocessed_text_dist_count'] / x['preprocessed_text_count'], axis=1)\n",
    "test_df['mistakes_dist_ratio'] = test_df.apply(lambda x: x['mistakes_dist_count'] / x['preprocessed_text_count'], axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n",
    "\n",
    "## Text Statistics Features\n",
    "print(\"Working on Text Statistics Features\")\n",
    "start_time = time.time()\n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(textstat_features)(text) for text in test_df['full_text_without_non_ascii'])\n",
    "features_df = pd.DataFrame(results)\n",
    "test_df = pd.concat([test_df, features_df], axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d044813-3eae-4d4c-b17f-5dc2a8f24584",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7098d7b9-b02f-49f6-b8c3-36c0fc43e422",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    'reading_time',\n",
    "    'mistakes_dist_ratio',\n",
    "    'polysyllabcount',\n",
    "    'sentence_count',\n",
    "    'difficult_words',\n",
    "    'comma_count',\n",
    "    'transitional_phrases_c',\n",
    "    'text_dist_words_ratio'\n",
    "]\n",
    "# Create a DataFrame for numerical features\n",
    "test_numerical_features_df = test_df[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7be1c66e-6960-43eb-8bb2-38d1ab8e04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling for test data completed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Apply the pre-trained scaler to the numerical features of the test dataset\n",
    "scaled_test_features = scaler.transform(test_numerical_features_df)\n",
    "\n",
    "# Convert scaled numerical features back to DataFrame\n",
    "scaled_test_features_df = pd.DataFrame(scaled_test_features, columns=numerical_features)\n",
    "\n",
    "print(\"Scaling for test data completed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e639c07-dca9-4021-9b76-81bf4d3ea153",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a78c71be-a380-41b0-97b4-565a3665ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF and PCA transformation for test data completed\n"
     ]
    }
   ],
   "source": [
    "# Apply the TF-IDF vectorizer to the test data\n",
    "test_tfidf_vectors = tfidf_vectorizer.transform(test_df['clean_lemm_preprocessed_text'])\n",
    "\n",
    "# Apply the PCA to the TF-IDF vectors of the test data\n",
    "test_tfidf_vectors_reduced = pca.transform(test_tfidf_vectors.toarray())\n",
    "\n",
    "# Create a DataFrame with the reduced TF-IDF features for the test data\n",
    "test_tfidf_features_df = pd.DataFrame(test_tfidf_vectors_reduced, columns=[f'tfidf_feature_{i}' for i in range(1, test_tfidf_vectors_reduced.shape[1] + 1)])\n",
    "\n",
    "print(\"TF-IDF and PCA transformation for test data completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcf534-6366-4968-800b-cd6559eb6433",
   "metadata": {},
   "source": [
    "### Combine Numerical and TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56308b8e-172f-44c0-8dc0-8ea8259ad9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features for test data: 508\n"
     ]
    }
   ],
   "source": [
    "# Combine the scaled numerical features with the TF-IDF features\n",
    "combined_test_features_df = pd.concat([scaled_test_features_df, test_tfidf_features_df], axis=1)\n",
    "\n",
    "# Print the count of the combined features\n",
    "print(f\"Total number of features for test data: {combined_test_features_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02099635-5ad8-48a3-a8e4-0a4b38842609",
   "metadata": {},
   "source": [
    "# Predicting and Discretizing Test Data via CatBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bfcce62-91d3-483a-924d-b65a8872eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted results on the test data:\n",
      "      Predicted Score\n",
      "0                   5\n",
      "1                   5\n",
      "2                   3\n",
      "3                   5\n",
      "4                   2\n",
      "...               ...\n",
      "4509                2\n",
      "4510                5\n",
      "4511                2\n",
      "4512                4\n",
      "4513                3\n",
      "\n",
      "[4514 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to discretize predictions\n",
    "def discretize_predictions(predictions, target_classes):\n",
    "    bins = np.linspace(min(target_classes) - 0.5, max(target_classes) + 0.5, num=len(target_classes) + 1)\n",
    "    discretized = np.digitize(predictions, bins) - 1\n",
    "    discretized = np.clip(discretized, 0, len(target_classes) - 1)\n",
    "    return discretized + 1\n",
    "\n",
    "# Custom scorer\n",
    "def qwk_scorer(y_true, y_pred):\n",
    "    target_classes = np.sort(np.unique(y_true))\n",
    "    y_pred_discretized = discretize_predictions(y_pred, target_classes)\n",
    "    return cohen_kappa_score(y_true, y_pred_discretized, weights='quadratic')\n",
    "    \n",
    "# Ensure the target classes are consistent with the training data\n",
    "target_classes = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = cb_reg.predict(combined_test_features_df)\n",
    "y_test_pred_discretized = discretize_predictions(y_test_pred, target_classes)\n",
    "\n",
    "# Print the predicted results\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Predicted Score': y_test_pred_discretized\n",
    "})\n",
    "print(\"Predicted results on the test data:\")\n",
    "print(test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8584-d4f8-47a8-a6af-782518f212ff",
   "metadata": {},
   "source": [
    "# Evaluate Result on Unseen Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6645d704-7bf7-42a0-9fdf-4a30218f0347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted results on the test data:\n",
      "      Actual Score  Predicted Score\n",
      "0                6                5\n",
      "1                4                5\n",
      "2                2                3\n",
      "3                4                5\n",
      "4                2                2\n",
      "...            ...              ...\n",
      "4509             2                2\n",
      "4510             6                5\n",
      "4511             3                2\n",
      "4512             4                4\n",
      "4513             2                3\n",
      "\n",
      "[4514 rows x 2 columns]\n",
      "Quadratic Weighted Kappa Score: 0.8359\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance\n",
    "y_test_actual = test_df['score']\n",
    "qwk_score = qwk_scorer(y_test_actual, y_test_pred_discretized)\n",
    "\n",
    "# Print the predicted results\n",
    "test_results_df = pd.DataFrame({\n",
    "    'Actual Score': y_test_actual,\n",
    "    'Predicted Score': y_test_pred_discretized\n",
    "})\n",
    "print(\"Predicted results on the test data:\")\n",
    "print(test_results_df)\n",
    "\n",
    "# Print the QWK score\n",
    "print(f\"Quadratic Weighted Kappa Score: {qwk_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c02c5-4fbb-4ede-bb2e-e142ab1f64db",
   "metadata": {},
   "source": [
    "# Traceability Matrix and Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0afca558-84e9-457a-abb2-707cb0829cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (DataFrame):\n",
      "   1     2    3    4    5   6\n",
      "1  1   204   44   18    0   0\n",
      "2  0  1055  241   44    2   0\n",
      "3  0   258  706  270   22   0\n",
      "4  0     4  188  461  132   0\n",
      "5  0     0    1   90  591   4\n",
      "6  0     0    0    0  160  18\n",
      "\n",
      "Detailed Analysis (DataFrame):\n",
      "    Actual  Predicted  Count\n",
      "0        1          2    204\n",
      "1        1          3     44\n",
      "2        1          4     18\n",
      "3        1          5      0\n",
      "4        1          6      0\n",
      "5        2          1      0\n",
      "6        2          3    241\n",
      "7        2          4     44\n",
      "8        2          5      2\n",
      "9        2          6      0\n",
      "10       3          1      0\n",
      "11       3          2    258\n",
      "12       3          4    270\n",
      "13       3          5     22\n",
      "14       3          6      0\n",
      "15       4          1      0\n",
      "16       4          2      4\n",
      "17       4          3    188\n",
      "18       4          5    132\n",
      "19       4          6      0\n",
      "20       5          1      0\n",
      "21       5          2      0\n",
      "22       5          3      1\n",
      "23       5          4     90\n",
      "24       5          6      4\n",
      "25       6          1      0\n",
      "26       6          2      0\n",
      "27       6          3      0\n",
      "28       6          4      0\n",
      "29       6          5    160\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# Confusion Matrix and Detailed Analysis\n",
    "conf_matrix = confusion_matrix(y_test_actual, y_test_pred_discretized, labels=target_classes)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=target_classes, columns=target_classes)\n",
    "\n",
    "# Display the confusion matrix as a DataFrame\n",
    "print(\"Confusion Matrix (DataFrame):\")\n",
    "print(conf_matrix_df)\n",
    "\n",
    "# Detailed Prediction Analysis\n",
    "detailed_analysis = []\n",
    "for i, true_label in enumerate(target_classes):\n",
    "    for j, pred_label in enumerate(target_classes):\n",
    "        if true_label != pred_label:\n",
    "            count = conf_matrix[i, j]\n",
    "            detailed_analysis.append({\n",
    "                'Actual': true_label,\n",
    "                'Predicted': pred_label,\n",
    "                'Count': count\n",
    "            })\n",
    "\n",
    "detailed_analysis_df = pd.DataFrame(detailed_analysis)\n",
    "\n",
    "# Display the detailed analysis as a DataFrame\n",
    "print(\"\\nDetailed Analysis (DataFrame):\")\n",
    "print(detailed_analysis_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621b4e6-63bc-4e75-b44e-0c4e8379818d",
   "metadata": {},
   "source": [
    "The QWK score on the unseen data set is 0.8359, which is consistent with the QWK scores observed during the training and validation process. This indicates that the model has generalized well to the unseen data.\n",
    "\n",
    "The detailed analysis provides insights into specific misclassifications. Notable points include:\n",
    "\n",
    "- The majority of instances are correctly classified, especially for classes 2 and 3.\n",
    "- Some instances of class 1 are misclassified as class 2 (204) and class 3 (44).\n",
    "- Some instances of class 3 are misclassified as class 2 (258) and class 4 (270).\n",
    "- Misclassifications are less frequent for higher classes (5 and 6), but there are still some notable errors, such as class 6 being predicted as class 5 (160 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43e38e-fca7-445e-9b94-e7beb2cf5049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
